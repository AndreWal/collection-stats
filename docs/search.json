[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "But what does it mean?",
    "section": "",
    "text": "0.1 How to read\nWelcome to the book. This is the landing page for the HTML version.\nUse the navigation sidebar to move between chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>But what does it mean?</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "2.1 Prerequisites\nThis preface explains why the book exists and how it is organized.\nHigh school level math. It is helpful if you have some experience with R so you understand the code snippets, but it is not necessary to understand the issues that I try to address in this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-this-book",
    "href": "preface.html#how-to-use-this-book",
    "title": "2  Preface",
    "section": "2.2 How to use this book",
    "text": "2.2 How to use this book\nAll data used in this book is simulated. Even though the goal of the book is not to demonstrate how to use a statistical software, code snippets are included to illustrate how to implement the methods discussed in the book. The code is written in R, but the concepts can be applied in other programming languages as well. The code snippets are meant to be illustrative and are not intended to be comprehensive or optimized for performance. Readers are encouraged to experiment with the code and adapt it to their own needs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1 Why this book\nWhy another book on statistics? There are already many excellent introductions, both technical and non-technical, and many are freely available online. Some focus on the mathematical foundations of statistics, while others provide practical guidance on applying statistical methods across contexts or implementing them in software such as R, Python, or Stata. Still others are tailored to specific disciplines and industries such as health and biostatistics, engineering, economics, and business. So why write another one?\nThere are two reasons why I decided to write this book. First, coding and implementing statistics into software is a skill that is likely to become less important with the arrival of AI-coding assistants. Most agents such as Claude Code or Codex can write better code in R, SQL, Python or other languages than most humans, and they can do it much faster. While some coding skills will still be necessary to understand how to use these tools as well as to debug and adjust code for specific use cases, the need to write code from scratch will likely diminish over time. What will become more important, however, is the ability to clearly communicate what a user wants an agent to do and how to understand and interpret the results that an agent produces.\nThis brings me to the second reason for writing this book. The book is also going to be the book that I hoped I had when I was learning statistics and doing research as a PhD student. While there were many great books on statistics already available, I found that there were a lot of open questions when doing quantitative research that were not covered in these books.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#roadmap",
    "href": "chapters/01-introduction.html#roadmap",
    "title": "3  Introduction",
    "section": "3.2 Roadmap",
    "text": "3.2 Roadmap\nOutline the main themes and how the chapters connect.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html",
    "href": "chapters/02-chapter-one.html",
    "title": "4  Reference Categories and Baseline Groups",
    "section": "",
    "text": "4.1 Categorical Predictor Variables\nWe already talked about how to interpret coefficients in a regression model. What we have not discussed is that the interpretation that a coefficient as the expected change in the response variable for a one-unit increase in the predictor variable, holding all other predictors constant, only applies to continuous predictor variables. For categorical predictors and other kind of bounded variables, the interpretation is different.\nLet’s imagine we investigate the relationship between gender and political donations. We have a binary variable gender that takes the value of 0 for men and 1 for women. We run a regression model with gender as the predictor variable and donationAmount as the response variable:\nsummary(lm(donationAmount ~ factor(gender), data = data))\n\n\nCall:\nlm(formula = donationAmount ~ factor(gender), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-88.756 -18.980   0.416  20.797  98.654 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       98.754      1.807   54.65   &lt;2e-16 ***\nfactor(gender)1   49.464      2.636   18.77   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.42 on 498 degrees of freedom\nMultiple R-squared:  0.4142,    Adjusted R-squared:  0.413 \nF-statistic: 352.1 on 1 and 498 DF,  p-value: &lt; 2.2e-16\nThe result shows that the coefficient for gender with the value 1 is 50. However, what does that mean? Since no one can become one unit more woman, another interpretation is needed. In case of a binary variable, the interpretation is straightforward: The coefficient of the group that is in the model specification is compared to the group that is not in the model specification. In this case, the coefficient of 50 for gender1 means that women donate on average 50 dollars more than men. The group that is not in the model specification, in this case men, is called the reference category or baseline group. The choice of the reference category is arbitrary and does not affect the overall fit of the model, but it does affect the interpretation of the coefficients.\nThe logic becomes a bit more complex when we have more than two categories. For instance, let’s assume we have another categorical variable called party identification with four possible values: Left, Center, Right, and Other. To estimate the effect of a respondent’s party identification on donationAmount, we need to choose at least one of the categories as the reference category. To illustrate the problem, we specify one regression model with Other as the reference category and another regression model with both Right and Other as the reference categories:\n# Leave out \"Other\" as the reference category\nmodel_other_ref &lt;- lm(donationAmount ~ left + center + right, data = data_party)\n\n# Leave out \"Right\" and \"Other\" as reference categories\nmodel_ro_ref &lt;- lm(donationAmount ~ left + center, data = data_party)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_other &lt;- coef(summary(model_other_ref))\ncs_ro_ref &lt;- coef(summary(model_ro_ref))\n\nterms &lt;- c(\"Left\", \"Center\", \"Right\", \"Other\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `Ref:Other` = c(\n    fmt_coef(cs_other, \"left\"),\n    fmt_coef(cs_other, \"center\"),\n    fmt_coef(cs_other, \"right\"),\n    \"ref\"\n  ),\n  `Ref:Right_Other` = c(\n    fmt_coef(cs_ro_ref, \"left\"),\n    fmt_coef(cs_ro_ref, \"center\"),\n    \"ref\",\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party effects with different reference categories\"\n)\n\n\nParty effects with different reference categories\n\n\nTerm\nRef.Other\nRef.Right_Other\n\n\n\n\nLeft\n39.77 (3.83)\n23.11 (2.65)\n\n\nCenter\n14.31 (3.75)\n-2.35 (2.54)\n\n\nRight\n23.02 (3.91)\nref\n\n\nOther\nref\nref\nThe first column of the tables shows the results when Other is the reference category. This means the coefficients for Left, Center, and Right are interpreted relative to Other category. For instance, the coefficient for Left with a value of 40 means that voters who identify as Left donate on average 40 dollars more than voters who identify as Other. However, the coefficient for Left changes considerably when we leave out the Right category as well. The reason is that the coefficient for Left in the second column has to be interpreted relative to the Other and Right category. Voters who identify as Right donate on average 23 dollars more than voters who identify as Other (see first column) and thus have the second largest coefficient in the first column. Consequently, leaving the Right category out, the coefficient for Left in the second column has now to be compared against Other and the - generously donating - Right category. As a result, the coefficient for Left becomes much smaller in the second column.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reference Categories and Baseline Groups</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html",
    "href": "chapters/03-chapter-two.html",
    "title": "5  Does it work? Well, it depends…",
    "section": "",
    "text": "5.1 Interaction Terms as Context\nResearch, data analysts, and their stakeholders often want to know if an intervention works in different contexts. An example is a political campaign manager might want to know if a particular message resonates differently with different demographic groups or geographic regions. Depending on the answer, financial resources might be allocated differently. For instance, the manager might decide to run online ads instead of mailers to reach younger voters. Alternatively, he might target low-turnout neighborhoods to mobilize voters in competitive districts. However, to make an informed decision, the manager needs to understand what the data says about the interaction of political ads and certain contexts. This is where interaction terms come in.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Does it work? Well, it depends...</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#what-is-an-interaction-term",
    "href": "chapters/03-chapter-two.html#what-is-an-interaction-term",
    "title": "5  Does it work? Well, it depends…",
    "section": "5.2 What is an Interaction Term?",
    "text": "5.2 What is an Interaction Term?\nWe established in the previous chapter that a coefficient in a regression model represents the average effect of a predictor variable on the response variable, holding all other variables constant. However, this assumes that the effect of the predictor is the same across all contexts. Using a linear probability model with a binary response variable voteDecision and a binary predictor variable politicalAd, we can illustrate this point. Mathematically, our model without an interaction term looks as follows:\n\\[\nvoteDecision_i = \\alpha + \\beta_1 politicalAd_i + \\epsilon_i\n\\tag{5.1}\\]\nAs we know from the previous chapters, the effect size of a predictor variable in a linear regression model is represented by the coefficient \\(\\beta_1\\). To understand interactions, it is helpful to know that the effect size of a predictor can simply read off the equation because it is the partial derivative of the response variable with respect to the predictor variable. In our simple regression model, the effect size of politicalAd is \\(\\beta_1\\), which means that for every one-unit increase in politicalAd, the expected change in voteDecision is \\(\\beta_1\\) units, holding all other variables constant. Mathematically, the partial derivative of Equation 5.1 can be expressed as: \\[\n\\frac{\\partial voteDecision_i}{\\partial politicalAd_i} = \\beta_1\n\\tag{5.2}\\]\nIn R, this model can be estimated using the lm() function as follows (using some simulated data):\n\nsummary(\n    lm(voteDecision ~ politicalAd, data = data)\n    )\n\n\nCall:\nlm(formula = voteDecision ~ politicalAd, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4889 -0.4889 -0.4056  0.5111  0.5944 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.40557    0.02211  18.341  &lt; 2e-16 ***\npoliticalAd  0.08337    0.03137   2.658  0.00799 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4959 on 998 degrees of freedom\nMultiple R-squared:  0.007029,  Adjusted R-squared:  0.006034 \nF-statistic: 7.064 on 1 and 998 DF,  p-value: 0.007989\n\n\nSo, why is this important to understand interaction terms? An interaction term is essentially a multiplicative combination of two predictor variables. When we include an interaction term in our regression model, we simply add two predictors and their product to the equation. Let’s assume that we want to know whether the effect of politicalAd on voteDecision is dependent on the age of a respondent. Our regression model with the interaction term would look like this: \\[\nvoteDecision_i = \\alpha + \\beta_1 politicalAd_i + \\beta_2 age_i + \\beta_3 (politicalAd_i \\times age_i) + \\epsilon_i\n\\tag{5.3}\\]\nIf we take the partial derivative of Equation 5.3 with respect to politicalAd, we get: \\[\n\\frac{\\partial voteDecision_i}{\\partial politicalAd_i} = \\beta_1 + \\beta_3 age_i\n\\tag{5.4}\\]\nAs Equation 5.4 shows, the effect of politicalAd on voteDecision is not constant but depends on the value of age. The coefficient \\(\\beta_3\\) represents the change in the effect of politicalAd for each one-unit increase in age. If \\(\\beta_3\\) is positive, it means that the effect of politicalAd on voteDecision increases as age increases. Conversely, if \\(\\beta_3\\) is negative, it means that the effect of politicalAd on voteDecision decreases as age increases.\nIn R, this would look like this:\n\nsummary(    \nlm(voteDecision ~ politicalAd * age, data = data)\n)\n\n\nCall:\nlm(formula = voteDecision ~ politicalAd * age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5618 -0.4550 -0.3224  0.5214  0.7091 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.223920   0.064736   3.459 0.000565 ***\npoliticalAd      0.381655   0.091075   4.191 3.03e-05 ***\nage              0.003673   0.001231   2.983 0.002920 ** \npoliticalAd:age -0.006093   0.001753  -3.477 0.000530 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4933 on 996 degrees of freedom\nMultiple R-squared:  0.0195,    Adjusted R-squared:  0.01654 \nF-statistic: 6.602 on 3 and 996 DF,  p-value: 0.0002034\n\n\n(R adds the interaction term automatically when you use the * operator between two variables in the formula.)\nEven thought it is more informative to plot the interaction to understand how the effect of politicalAd changes across different ages, we can also look at the coefficients to get a sense of the interaction. We can see that the coefficient for politicalAd is positive. However, the coefficient politicalAd gives us only the effect of politicalAd when age is zero, which is not a meaningful value in this context. The coefficient for the interaction term politicalAd:age is negative, which indicates that the effect of politicalAd on voteDecision decreases as age increases. This means that the political ad is more effective for younger voters than for older voters. Just looking at the table gives us already a pretty good idea of the interaction, but it is often more intuitive to visualize it:\n\n# Fit the model with interaction\nmodel &lt;- lm(voteDecision ~ politicalAd * age, data = data)\n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model)\nvcov_matrix &lt;- vcov(model)\n\n# Create a sequence for the moderator variable (age)\nage_seq &lt;- seq(18, 80, length.out = 100)\n\neffect &lt;- coefs[2] + coefs[4] * age_seq\n\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           age_seq^2 * vcov_matrix[4, 4] + \n           2 * age_seq * vcov_matrix[2, 4])\n\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(age_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n     xlab = \"Age\",\n     ylab = \"Effect of Political Ad on Vote Decision\",\n     main = \"Marginal Effect of Political Ads by Age\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(age_seq, rev(age_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.2, 0.2, 0.8, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(age_seq, effect, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nUsing the model with the interaction term and the delta method, we can plot the marginal effect of politicalAd on voteDecision across different ages, along with 95% confidence intervals. The plot shows that the change in percentage points of the political ad is positive for younger voters but decreases as age increases, eventually becoming no longer distinguishable from zero for voters older than 50. This visualization gives us a much clearer picture of how the effect of the political ad changes across different ages compared to a regression table.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Does it work? Well, it depends...</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#a-brief-digression-polynomial-regression",
    "href": "chapters/03-chapter-two.html#a-brief-digression-polynomial-regression",
    "title": "5  Does it work? Well, it depends…",
    "section": "5.3 A Brief Digression: Polynomial Regression",
    "text": "5.3 A Brief Digression: Polynomial Regression\nSince we have already introduced the concept of interaction terms, it is worth mentioning that polynomial regression is a special case of interaction terms. Even though the link function between the predictor and the response variable is linear in polynomial regression, the relationship between the predictor and the response variable can be non-linear. To model non-linear relationships, we use interaction terms as well. In contrast to our previous example, however, we do not interact two different predictor variables but rather a predictor variable with itself. This allows us to capture non-linear relationships between the predictor and the response variable. Using age as an example, our regression model with a cubic term would look like this: \\[\nvoteDecision_i = \\alpha + \\beta_1 age_i + \\beta_2 age_i^2 + \\beta_3 age_i^3 + \\epsilon_i\n\\tag{5.5}\\]\nThe partial derivative of Equation 5.5 with respect to age is: \\[\n\\frac{\\partial voteDecision_i}{\\partial age_i} = \\beta_1 + 2 \\beta_2 age_i + 3 \\beta_3 age_i^2\n\\tag{5.6}\\]\nIn R, a polynomial regression can be estimated using the poly() function as follows:\n\nsummary(\n    lm(voteDecision ~ poly(age, degree = 3), data = data2)\n    )\n\n\nCall:\nlm(formula = voteDecision ~ poly(age, degree = 3), data = data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49942 -0.04568 -0.00698  0.00680  0.98670 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.07600    0.00729  10.425  &lt; 2e-16 ***\npoly(age, degree = 3)1 -3.20179    0.23053 -13.889  &lt; 2e-16 ***\npoly(age, degree = 3)2  2.39403    0.23053  10.385  &lt; 2e-16 ***\npoly(age, degree = 3)3 -1.14468    0.23053  -4.965 8.06e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2305 on 996 degrees of freedom\nMultiple R-squared:  0.2463,    Adjusted R-squared:  0.244 \nF-statistic: 108.5 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the plot of the marginal effect of age on voteDecision would look like this:\n\n# Fit the polynomial regression model with raw polynomials\nmodel_poly &lt;- lm(voteDecision ~ age + I(age^2) + I(age^3), data = data2) \n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model_poly)\nvcov_matrix &lt;- vcov(model_poly)\n\n# Create age sequence for prediction\nage_seq &lt;- seq(18, 80, length.out = 100)\n\n# Calculate marginal effect: β₁ + 2*β₂*age + 3*β₃*age²\n# coefs[2] = β₁ (age)\n# coefs[3] = β₂ (age²)\n# coefs[4] = β₃ (age³)\neffect &lt;- coefs[2] + 2 * coefs[3] * age_seq + 3 * coefs[4] * age_seq^2\n\n# Delta method for standard error:\n# For f(β) = β₁ + 2*β₂*age + 3*β₃*age², gradient is [1, 2*age, 3*age²]\n# SE = sqrt(gradient' * Vcov * gradient)\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           4 * age_seq^2 * vcov_matrix[3, 3] + \n           9 * age_seq^4 * vcov_matrix[4, 4] +\n           4 * age_seq * vcov_matrix[2, 3] +\n           6 * age_seq^2 * vcov_matrix[2, 4] +\n           12 * age_seq^3 * vcov_matrix[3, 4])\n\n# Calculate 95% confidence intervals\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(age_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n     xlab = \"Age\",\n     ylab = \"Marginal Effect of Age on Vote Decision\",\n     main = \"Marginal Effect of Age\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(age_seq, rev(age_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.8, 0.2, 0.2, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(age_seq, effect, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\nThe plot shows that the marginal effect of age on voteDecision is negative for younger voters but diminishes over time and becomes largely insignificant for voters in the mid-40s and older.\nOf course, other functional forms could be considered to capture different types of non-linear relationships. But the main point is that this example illustrates how polynomial regression can capture non-linear relationships between a predictor variable and a response variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Does it work? Well, it depends...</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html#shares",
    "href": "chapters/02-chapter-one.html#shares",
    "title": "4  Reference Categories and Baseline Groups",
    "section": "4.2 Shares",
    "text": "4.2 Shares\nOne common case that is subject to the same problem of reference categories but is rarely discussed in the literature is the case of shares. For instance, researchers are often interested in the effect of the share of the population living in urban areas, the share of the agricultural or industrial work force, the share of ethnic or religious groups, or the share of the population with a certain level of education on some outcome (to name but a few examples). In all these cases, the share variable is bounded between 0 and 1 and thus has a reference category at both ends of the spectrum. For instance, if we have a variable that measures the share of the population living in urban areas, then the reference category for a value of 0 is that no one lives in urban areas (and everybody lives in rural areas), while the reference category for a value of 1 is that everyone lives in urban areas (and no one lives in rural areas). This means that the interpretation of the coefficients for share variables can be quite complex and depends on the number of categories.\nA quite prominent example is the literature on the relationship between the ideological composition of government and spending patterns. A typical setup is to use total government spending or social security spending as the response variable and the share of specific parties in government as the predictor variable. Here again, the interpretation of the coefficient for the share of specific parties in government depends on the reference category. In some specifications, only the share of left-wing parties is included as a predictor variable, while in other specifications, the share of left-wing and center or Christian parties is included. The interpretation of the coefficient for the share of left-wing parties in government changes considerably depending on whether the share of other types of parties is included in the model specification or left out as reference category. Thus, the interpretation of the coefficients for share variables is the same as for categorical variables with the difference that share variables can take on a continuous range of values between 0 and 1, while categorical variables take on discrete values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reference Categories and Baseline Groups</span>"
    ]
  }
]