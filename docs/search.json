[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "But what does it mean?",
    "section": "",
    "text": "0.1 How to read\nWelcome to the book. This is the landing page for the HTML version.\nUse the navigation sidebar to move between chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>But what does it mean?</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "2.1 Prerequisites\nThis preface explains why the book exists and how it is organized.\nHigh school level math. It is helpful if you have some experience with R so you understand the code snippets, but it is not necessary to understand the issues that I try to address in this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-this-book",
    "href": "preface.html#how-to-use-this-book",
    "title": "2  Preface",
    "section": "2.2 How to use this book",
    "text": "2.2 How to use this book\nAll data used in this book is simulated. Even though the goal of the book is not to demonstrate how to use a statistical software, code snippets are included to illustrate how to implement the methods discussed in the book. The code is written in R, but the concepts can be applied in other programming languages as well. The code snippets are meant to be illustrative and are not intended to be comprehensive or optimized for performance. Readers are encouraged to experiment with the code and adapt it to their own needs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1 Why this book\nWhy another book on statistics? There are already many excellent introductions, both technical and non-technical, and many are freely available online. Some focus on the mathematical foundations of statistics, while others provide practical guidance on applying statistical methods across contexts or implementing them in software such as R, Python, or Stata. Still others are tailored to specific disciplines and industries such as health and biostatistics, engineering, economics, and business. So why write another one?\nThere are two reasons why I decided to write this book. First, coding and implementing statistics into software is a skill that is likely to become less important with the arrival of AI-coding assistants. Most agents such as Claude Code or Codex can write better code in R, SQL, Python or other languages than most humans, and they can do it much faster. While some coding skills will still be necessary to understand how to use these tools as well as to debug and adjust code for specific use cases, the need to write code from scratch will likely diminish over time. What will become more important, however, is the ability to clearly communicate what a user wants an agent to do and how to understand and interpret the results that an agent produces.\nThis brings me to the second reason for writing this book. The book is also going to be the book that I hoped I had when I was learning statistics and doing research as a PhD student. While there were many great books on statistics already available, I found that there were a lot of open questions when doing quantitative research that were not covered in these books.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#roadmap",
    "href": "chapters/01-introduction.html#roadmap",
    "title": "3  Introduction",
    "section": "3.2 Roadmap",
    "text": "3.2 Roadmap\nOutline the main themes and how the chapters connect.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html",
    "href": "chapters/02-chapter-one.html",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "",
    "text": "4.1 Binary Predictor Variables\nI already discussed how to interpret coefficients in a regression model. What I have not discussed is that the interpretation of a coefficient as the expected change in the response variable for a one-unit increase in the predictor variable (holding all other predictors constant) is most direct when a one-unit change is substantively meaningful. For binary and categorical predictors, coefficients are instead interpreted as differences between categories defined by the coding scheme. This has consequences for how we specify models and interpret results as I will show in the following.\nSuppose I study the relationship between gender and political donations. I have a binary variable gender coded 0 for men and 1 for women and donationAmount in US dollars. To understand how categorical variables work in regression analysis, I start by computing the average donation amount for men and women:\nmean_donation &lt;- aggregate(donationAmount ~ gender, data = data, FUN = mean)\nmean_donation$gender &lt;- c(\"Men\", \"Women\")\ncolnames(mean_donation) &lt;- c(\"Gender\", \"Mean Donation Amount\")\nknitr::kable(mean_donation, digits = 2)\n\n\n\n\nGender\nMean Donation Amount\n\n\n\n\nMen\n98.75\n\n\nWomen\n148.22\nAs the table shows, men donate on average 98.75 dollars, while women donate on average 148.22 dollars. With that in mind, I estimate a regression model with donationAmount as the response variable and gender as the predictor variable. Mathematically, the model can be written as follows:\n\\[donationAmount_i = \\alpha_0 + \\beta_1 gender_i + \\epsilon_i\\]\nIn R, I estimate the model using the lm() function:\nlm(donationAmount ~ factor(gender), data = data)\n\n\nCall:\nlm(formula = donationAmount ~ factor(gender), data = data)\n\nCoefficients:\n    (Intercept)  factor(gender)1  \n          98.75            49.46\nI start with the interpretation of the results before digging deeper. The coefficient for gender is about 50. In the case of a binary variable, the interpretation is straightforward: The coefficient compares the coded group (1) to the reference group (0). More precisely, the coefficient of about 50 for gender means that women donate on average 50 dollars more than men.\nComparing the regression output to the means table, I can highlight two points. First, the coefficient of the intercept is about 98.75, which is the average donation amount for men. Put differently, the intercept equals the mean of the reference category of men in this specification because, more generally, the intercept is the expected outcome when all predictors are zero and factors are at their baseline levels. This can be expressed mathematically:\n\\[E(donationAmount \\mid \\text{men}) = \\alpha_0\\]\nThis is why, when the model includes an intercept, one category must be omitted; otherwise the full set of dummies is perfectly collinear with the intercept (the dummy-variable trap). That means that if I included binary variables for both men and women in the model, the dummy variables would be perfectly collinear with the intercept and the model would not be estimable.\nSecond, the coefficient for gender is about 50, which is the difference in mean donation amounts between men and women. Thus, the coefficient for gender does not give us the average donation amount for women, but rather the difference in average donation amounts between women and men. In turn, this means that the mean donation amount of women is the sum of the coefficient of the intercept and the coefficient for gender:\n\\[E(donationAmount \\mid \\text{women}) = \\alpha_0 + \\beta_1\\]\nLastly, the substantive interpretation of the results does not change if I alter the reference category. With pure releveling (changing only which category is omitted), fitted values and model fit stay the same, but coefficient labels and interpretations change. To illustrate this, I can relevel the gender variable so that women are the reference category and are omitted from the model:\nlm(donationAmount ~ relevel(factor(gender), ref = \"1\"), data = data)\n\n\nCall:\nlm(formula = donationAmount ~ relevel(factor(gender), ref = \"1\"), \n    data = data)\n\nCoefficients:\n                        (Intercept)  relevel(factor(gender), ref = \"1\")0  \n                             148.22                               -49.46\nAs shown, the coefficient for intercept is now about 148, which is equal to the average donation amount for women according to the means table. The coefficient for gender (level 0) now displays the difference in donation amount of men compared to women. More precisely, it is now about -50, meaning that men donate on average 50 dollars less than women. Again, summing the coefficient of the intercept and the coefficient for gender gives the average donation amount for men. Thus, while the coefficients of intercept and gender have changed, the substantive interpretation of the results has not. The choice of reference category does not affect the substantive interpretation of the results, but it does affect how the coefficients are read.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html",
    "href": "chapters/03-chapter-two.html",
    "title": "5  Does it work? Well, it depends…",
    "section": "",
    "text": "5.1 Interaction Terms as Context\nResearch, data analysts, and their stakeholders often want to know if an intervention works in different contexts. An example is a political campaign manager might want to know if a particular message resonates differently with different demographic groups or geographic regions. Depending on the answer, financial resources might be allocated differently. For instance, the manager might decide to run online ads instead of mailers to reach younger voters. Alternatively, he might target low-turnout neighborhoods to mobilize voters in competitive districts. However, to make an informed decision, the manager needs to understand what the data says about the interaction of political ads and certain contexts. This is where interaction terms come in.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Does it work? Well, it depends...</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#what-is-an-interaction-term",
    "href": "chapters/03-chapter-two.html#what-is-an-interaction-term",
    "title": "5  Does it work? Well, it depends…",
    "section": "5.2 What is an Interaction Term?",
    "text": "5.2 What is an Interaction Term?\nWe established in the previous chapter that a coefficient in a regression model represents the average effect of a predictor variable on the response variable, holding all other variables constant. However, this assumes that the effect of the predictor is the same across all contexts. Using a linear probability model with a binary response variable voteDecision and a binary predictor variable politicalAd, we can illustrate this point. Mathematically, our model without an interaction term looks as follows:\n\\[\nvoteDecision_i = \\alpha + \\beta_1 politicalAd_i + \\epsilon_i\n\\tag{5.1}\\]\nAs we know from the previous chapters, the effect size of a predictor variable in a linear regression model is represented by the coefficient \\(\\beta_1\\). To understand interactions, it is helpful to know that the effect size of a predictor can simply read off the equation because it is the partial derivative of the response variable with respect to the predictor variable. In our simple regression model, the effect size of politicalAd is \\(\\beta_1\\), which means that for every one-unit increase in politicalAd, the expected change in voteDecision is \\(\\beta_1\\) units, holding all other variables constant. Mathematically, the partial derivative of Equation 5.1 can be expressed as: \\[\n\\frac{\\partial voteDecision_i}{\\partial politicalAd_i} = \\beta_1\n\\tag{5.2}\\]\nIn R, this model can be estimated using the lm() function as follows (using some simulated data):\n\nsummary(\n    lm(voteDecision ~ politicalAd, data = data)\n    )\n\n\nCall:\nlm(formula = voteDecision ~ politicalAd, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4889 -0.4889 -0.4056  0.5111  0.5944 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.40557    0.02211  18.341  &lt; 2e-16 ***\npoliticalAd  0.08337    0.03137   2.658  0.00799 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4959 on 998 degrees of freedom\nMultiple R-squared:  0.007029,  Adjusted R-squared:  0.006034 \nF-statistic: 7.064 on 1 and 998 DF,  p-value: 0.007989\n\n\nSo, why is this important to understand interaction terms? An interaction term is essentially a multiplicative combination of two predictor variables. When we include an interaction term in our regression model, we simply add two predictors and their product to the equation. Let’s assume that we want to know whether the effect of politicalAd on voteDecision is dependent on the age of a respondent. Our regression model with the interaction term would look like this: \\[\nvoteDecision_i = \\alpha + \\beta_1 politicalAd_i + \\beta_2 age_i + \\beta_3 (politicalAd_i \\times age_i) + \\epsilon_i\n\\tag{5.3}\\]\nIf we take the partial derivative of Equation 5.3 with respect to politicalAd, we get: \\[\n\\frac{\\partial voteDecision_i}{\\partial politicalAd_i} = \\beta_1 + \\beta_3 age_i\n\\tag{5.4}\\]\nAs Equation 5.4 shows, the effect of politicalAd on voteDecision is not constant but depends on the value of age. The coefficient \\(\\beta_3\\) represents the change in the effect of politicalAd for each one-unit increase in age. If \\(\\beta_3\\) is positive, it means that the effect of politicalAd on voteDecision increases as age increases. Conversely, if \\(\\beta_3\\) is negative, it means that the effect of politicalAd on voteDecision decreases as age increases.\nIn R, this would look like this:\n\nsummary(    \nlm(voteDecision ~ politicalAd * age, data = data)\n)\n\n\nCall:\nlm(formula = voteDecision ~ politicalAd * age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5618 -0.4550 -0.3224  0.5214  0.7091 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      0.223920   0.064736   3.459 0.000565 ***\npoliticalAd      0.381655   0.091075   4.191 3.03e-05 ***\nage              0.003673   0.001231   2.983 0.002920 ** \npoliticalAd:age -0.006093   0.001753  -3.477 0.000530 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4933 on 996 degrees of freedom\nMultiple R-squared:  0.0195,    Adjusted R-squared:  0.01654 \nF-statistic: 6.602 on 3 and 996 DF,  p-value: 0.0002034\n\n\n(R adds the interaction term automatically when you use the * operator between two variables in the formula.)\nEven thought it is more informative to plot the interaction to understand how the effect of politicalAd changes across different ages, we can also look at the coefficients to get a sense of the interaction. We can see that the coefficient for politicalAd is positive. However, the coefficient politicalAd gives us only the effect of politicalAd when age is zero, which is not a meaningful value in this context. The coefficient for the interaction term politicalAd:age is negative, which indicates that the effect of politicalAd on voteDecision decreases as age increases. This means that the political ad is more effective for younger voters than for older voters. Just looking at the table gives us already a pretty good idea of the interaction, but it is often more intuitive to visualize it:\n\n# Fit the model with interaction\nmodel &lt;- lm(voteDecision ~ politicalAd * age, data = data)\n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model)\nvcov_matrix &lt;- vcov(model)\n\n# Create a sequence for the moderator variable (age)\nage_seq &lt;- seq(18, 80, length.out = 100)\n\neffect &lt;- coefs[2] + coefs[4] * age_seq\n\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           age_seq^2 * vcov_matrix[4, 4] + \n           2 * age_seq * vcov_matrix[2, 4])\n\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(age_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n     xlab = \"Age\",\n     ylab = \"Effect of Political Ad on Vote Decision\",\n     main = \"Marginal Effect of Political Ads by Age\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(age_seq, rev(age_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.2, 0.2, 0.8, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(age_seq, effect, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nUsing the model with the interaction term and the delta method, we can plot the marginal effect of politicalAd on voteDecision across different ages, along with 95% confidence intervals. The plot shows that the change in percentage points of the political ad is positive for younger voters but decreases as age increases, eventually becoming no longer distinguishable from zero for voters older than 50. This visualization gives us a much clearer picture of how the effect of the political ad changes across different ages compared to a regression table.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Does it work? Well, it depends...</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#a-brief-digression-polynomial-regression",
    "href": "chapters/03-chapter-two.html#a-brief-digression-polynomial-regression",
    "title": "5  Does it work? Well, it depends…",
    "section": "5.3 A Brief Digression: Polynomial Regression",
    "text": "5.3 A Brief Digression: Polynomial Regression\nSince we have already introduced the concept of interaction terms, it is worth mentioning that polynomial regression is a special case of interaction terms. Even though the link function between the predictor and the response variable is linear in polynomial regression, the relationship between the predictor and the response variable can be non-linear. To model non-linear relationships, we use interaction terms as well. In contrast to our previous example, however, we do not interact two different predictor variables but rather a predictor variable with itself. This allows us to capture non-linear relationships between the predictor and the response variable. Using age as an example, our regression model with a cubic term would look like this: \\[\nvoteDecision_i = \\alpha + \\beta_1 age_i + \\beta_2 age_i^2 + \\beta_3 age_i^3 + \\epsilon_i\n\\tag{5.5}\\]\nThe partial derivative of Equation 5.5 with respect to age is: \\[\n\\frac{\\partial voteDecision_i}{\\partial age_i} = \\beta_1 + 2 \\beta_2 age_i + 3 \\beta_3 age_i^2\n\\tag{5.6}\\]\nIn R, a polynomial regression can be estimated using the poly() function as follows:\n\nsummary(\n    lm(voteDecision ~ poly(age, degree = 3), data = data2)\n    )\n\n\nCall:\nlm(formula = voteDecision ~ poly(age, degree = 3), data = data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49942 -0.04568 -0.00698  0.00680  0.98670 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.07600    0.00729  10.425  &lt; 2e-16 ***\npoly(age, degree = 3)1 -3.20179    0.23053 -13.889  &lt; 2e-16 ***\npoly(age, degree = 3)2  2.39403    0.23053  10.385  &lt; 2e-16 ***\npoly(age, degree = 3)3 -1.14468    0.23053  -4.965 8.06e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2305 on 996 degrees of freedom\nMultiple R-squared:  0.2463,    Adjusted R-squared:  0.244 \nF-statistic: 108.5 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the plot of the marginal effect of age on voteDecision would look like this:\n\n# Fit the polynomial regression model with raw polynomials\nmodel_poly &lt;- lm(voteDecision ~ age + I(age^2) + I(age^3), data = data2) \n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model_poly)\nvcov_matrix &lt;- vcov(model_poly)\n\n# Create age sequence for prediction\nage_seq &lt;- seq(18, 80, length.out = 100)\n\n# Calculate marginal effect: β₁ + 2*β₂*age + 3*β₃*age²\n# coefs[2] = β₁ (age)\n# coefs[3] = β₂ (age²)\n# coefs[4] = β₃ (age³)\neffect &lt;- coefs[2] + 2 * coefs[3] * age_seq + 3 * coefs[4] * age_seq^2\n\n# Delta method for standard error:\n# For f(β) = β₁ + 2*β₂*age + 3*β₃*age², gradient is [1, 2*age, 3*age²]\n# SE = sqrt(gradient' * Vcov * gradient)\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           4 * age_seq^2 * vcov_matrix[3, 3] + \n           9 * age_seq^4 * vcov_matrix[4, 4] +\n           4 * age_seq * vcov_matrix[2, 3] +\n           6 * age_seq^2 * vcov_matrix[2, 4] +\n           12 * age_seq^3 * vcov_matrix[3, 4])\n\n# Calculate 95% confidence intervals\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(age_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n     xlab = \"Age\",\n     ylab = \"Marginal Effect of Age on Vote Decision\",\n     main = \"Marginal Effect of Age\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(age_seq, rev(age_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.8, 0.2, 0.2, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(age_seq, effect, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\nThe plot shows that the marginal effect of age on voteDecision is negative for younger voters but diminishes over time and becomes largely insignificant for voters in the mid-40s and older.\nOf course, other functional forms could be considered to capture different types of non-linear relationships. But the main point is that this example illustrates how polynomial regression can capture non-linear relationships between a predictor variable and a response variable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Does it work? Well, it depends...</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html#shares",
    "href": "chapters/02-chapter-one.html#shares",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "4.3 Shares",
    "text": "4.3 Shares\nAnother case that is subject to the same problem of reference categories but is often underemphasized in the literature is the case of shares. For instance, researchers are often interested in the effect of the share of the population living in urban areas, the share of the agricultural or industrial workforce, the share of ethnic or religious groups, or the share of the population with a certain level of education on some outcome (to name but a few examples). In all these cases, the share variable is bounded between 0 and 1 and thus has an implicit complement (e.g., rural share = 1 − urban share). For instance, if I have a variable that measures the share of the population living in urban areas, then a value of 0 means everyone is in the complement category, and a value of 1 means no one is in the complement category. This means that the interpretation of the coefficients for share variables is subject to the same problems as before and depends on the number of categories and on which share(s) are omitted from the model.\nTo illustrate the problem, consider party shares in government. Suppose I observe the share of cabinet seats held by left, center, and right parties. Each share is between 0 and 1 and the three shares sum to 1. Because the shares sum to 1, these relationships are always interpreted as shifts in composition (increasing one share necessarily decreases at least one other share). I also observe social spending per capita which serves as the response variable.\nBecause the shares sum to 1, one share must be omitted when I include an intercept. If I omit the right share, the model can be written as:\n\\[\nSocialSpendingpc_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\nHowever, there are a number of different ways to specify the model. Since I can no longer display the mean of the response variable by group (as I did above), I establish a baseline by estimating a model with all party groups and no intercept. After that, I can drop the right share which is the reference category in the first model to demonstrate how both models are connected.\n\nmodel_drop_right &lt;- lm(social_spend_pc ~ left + center, data = data_party_share)\nmodel_no_intercept &lt;- lm(social_spend_pc ~ left + center + right - 1, data = data_party_share)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_drop_right &lt;- coef(summary(model_drop_right))\ncs_no_intercept &lt;- coef(summary(model_no_intercept))\n\nterms &lt;- c(\"(Intercept)\", \"left\", \"center\", \"right\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `No_Intercept` = c(\n    \"—\",\n    fmt_coef(cs_no_intercept, \"left\"),\n    fmt_coef(cs_no_intercept, \"center\"),\n    fmt_coef(cs_no_intercept, \"right\")\n  ),\n  `No_Right` = c(\n    fmt_coef(cs_drop_right, \"(Intercept)\"),\n    fmt_coef(cs_drop_right, \"left\"),\n    fmt_coef(cs_drop_right, \"center\"),\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party-share regressions with different baselines\"\n)\n\n\nParty-share regressions with different baselines\n\n\nTerm\nNo_Intercept\nNo_Right\n\n\n\n\n(Intercept)\n—\n517.22 (120.67)\n\n\nleft\n6987.47 (114.32)\n6470.26 (199.61)\n\n\ncenter\n1460.77 (113.01)\n943.56 (194.27)\n\n\nright\n517.22 (120.67)\nref\n\n\n\n\n\nThe no-intercept model displays the expected level of social spending per capita if a given party held all cabinet seats (i.e., its share equals 1 and the other shares equal 0). For example, the coefficient for left of about 7000 means that if a government were composed entirely of left parties, I would expect social spending per capita to be almost 7000. Similarly, the coefficients for center and right show the expected spending levels under hypothetical all-center or all-right governments. This specification serves as a diagnostic baseline because it shows the direction and magnitude of each party share’s association with social spending.\nIn the second model with an intercept, the right shares variable is absorbed into the intercept, and each remaining coefficient becomes a contrast against the omitted share rather than a standalone predicted level. The coefficient for intercept is now the expected level of social spending per capita if a government were composed entirely of right parties (i.e., right share equals 1 and the other shares equal 0). The coefficients for left and center are now interpreted as the expected change in social spending per capita for a one-unit (i.e., 100 percentage point) increase in the left (center) share and a simultaneous one-unit decrease in the right share, holding the center (left) share variable constant. Thus, the relationship between the left share and social spending can be mathematically expressed as follows:\n\\[E(SocialSpendingpc \\mid left=1, center=0, right=0) = \\alpha_0 + \\beta_1\\]\nAdding the coefficient for the intercept and the coefficient for left in the second model reproduces the coefficient for left in the no-intercept model. This means that the expected level of social spending per capita if a government were composed entirely of left parties is the sum of the coefficient for intercept and the coefficient for left in the second model, which matches the coefficient for left in the no-intercept model. The same logic applies to the center share.\nThe example illustrates that models with share variables are subject to the same issues of reference categories as models with categorical predictors. The interpretation of the coefficients depends on which share(s) are omitted from the model, and the choice of reference category can change the substantive interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html#predictor-variables-with-more-than-two-categories",
    "href": "chapters/02-chapter-one.html#predictor-variables-with-more-than-two-categories",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "4.2 Predictor Variables with More than Two Categories",
    "text": "4.2 Predictor Variables with More than Two Categories\nThe logic becomes a bit more complex when I have more than two categories. For instance, suppose I have another categorical variable called party identification with four possible groups: Left, Center, Right, and Other. To illustrate the link between reference categories and group means, I display the average donation amount for each party identification group in the table below. In addition, I add a row for the average donation amount for the combined Right and Other group, which will help me to illustrate what changes when I omit multiple categories (instead of one category) in the regression model.\n\nmean_party &lt;- aggregate(donationAmount ~ party, data = data.frame(\n  party = party,\n  donationAmount = donationAmount\n), FUN = mean)\nmean_ro &lt;- mean(donationAmount[party %in% c(\"Right\", \"Other\")])\nmean_party &lt;- rbind(\n  mean_party,\n  data.frame(party = \"Right + Other\", donationAmount = mean_ro)\n)\ncolnames(mean_party) &lt;- c(\"Party\", \"Mean Donation Amount\")\nknitr::kable(mean_party, digits = 2)\n\n\n\n\nParty\nMean Donation Amount\n\n\n\n\nCenter\n115.45\n\n\nLeft\n140.90\n\n\nOther\n101.14\n\n\nRight\n124.15\n\n\nRight + Other\n117.80\n\n\n\n\n\nTo estimate the relationship of a respondent’s party identification and donationAmount in a linear regression model, I need to choose one of the categories as the reference category. To illustrate what happens when I omit more than one category, I specify one regression model with Other as the reference category and another regression model with Right and Other jointly omitted (a pooled reference category). Mathematically, the first model can be written as follows:\n\\[\ndonationAmount_i  = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\beta_3 right_i + \\epsilon_i\n\\]\nWith Right and Other pooled into a single reference category, the second model becomes:\n\\[\ndonationAmount_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\n\n# Leave out \"Other\" as the reference category\nmodel_other_ref &lt;- lm(donationAmount ~ left + center + right, data = data_party)\n\n# Leave out \"Right\" and \"Other\" as reference categories\nmodel_ro_ref &lt;- lm(donationAmount ~ left + center, data = data_party)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_other &lt;- coef(summary(model_other_ref))\ncs_ro_ref &lt;- coef(summary(model_ro_ref))\n\nterms &lt;- c(\"(Intercept)\", \"Left\", \"Center\", \"Right\", \"Other\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `Ref:Other` = c(\n    fmt_coef(cs_other, \"(Intercept)\"),\n    fmt_coef(cs_other, \"left\"),\n    fmt_coef(cs_other, \"center\"),\n    fmt_coef(cs_other, \"right\"),\n    \"ref\"\n  ),\n  `Ref:Right_Other` = c(\n    fmt_coef(cs_ro_ref, \"(Intercept)\"),\n    fmt_coef(cs_ro_ref, \"left\"),\n    fmt_coef(cs_ro_ref, \"center\"),\n    \"ref\",\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party effects with different reference categories\"\n)\n\n\nParty effects with different reference categories\n\n\nTerm\nRef.Other\nRef.Right_Other\n\n\n\n\n(Intercept)\n101.14 (3.32)\n117.80 (1.80)\n\n\nLeft\n39.77 (3.83)\n23.11 (2.65)\n\n\nCenter\n14.31 (3.75)\n-2.35 (2.54)\n\n\nRight\n23.02 (3.91)\nref\n\n\nOther\nref\nref\n\n\n\n\n\nThe first column of the table reports estimates with Other as the reference category, so the coefficients for Left, Center, and Right are each interpreted relative to Other. This means that the coefficient of the intercept is the average donation amount for respondents identifying as Other, and the coefficients for Left, Center, and Right show how much more (or less) respondents in these groups donate on average compared to respondents in Other. As shown before, the results can be easily cross-checked with the means table. While the coefficient for the intercept in the regression is equal to the mean donation amount for Other respondents in the means table, the coefficients for Left, Center, and Right have to be added to the intercept to get the mean donation amount for these groups, which matches the values in the means table as well. For example, the average donation amount for Left respondents in the first column can be deduced as follows: \\[E(donationAmount \\mid \\text{Left}) = \\alpha_0 + \\beta_1 \\]\nwhich is about 101.14 + 39.77 = 140.91 dollars, which matches roughly the average donation amount for Left respondents in the means table (aside from differences in rounding). The same logic can be applied to the coefficients for Center and Right.\nIn the second column, however, I change the specification by omitting Right as well, which means Right and Other are pooled into a single reference category. Thus, this is not merely releveling; it is a different model specification because it compares Left and Center to Right and Other. Coefficients therefore change meaning. While the interpretation is still straightforward as before, the estimand is different from the first model.\nMore precisely, the coefficient for the intercept in the second column is now the average donation amount for respondents identifying as Right or Other. This can be confirmed by comparing the coefficient of the intercept with the row for Right or Other in the means table. This also means that the coefficients for Left and Center are now interpreted as the difference in average donation amounts between respondents identifying as either Left or Center on one hand and respondents identifying as Right or Other on the other hand. And since the coefficient of the intercept increased by almost 17 dollars compared to the first column, the coefficients for Left and Center decreased accordingly. For instance, the coefficient for Center is now negative, which reflects the fact that the average donation amount for respondents identifying as Center is 2.35 dollars less than the average donation amount for respondents identifying as Right or Other as the means table shows.\nThe main takeaway is that with pure releveling, fitted values and model fit do not change; only coefficient labels and comparisons do. With pooling reference categories, you change the comparison group and therefore the estimand and the substantive conclusions that you can draw.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  }
]