[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "",
    "text": "0.1 Why this collection?\nThis collection is a work-in-progress. New articles will be added over time.\nWhy publish another set of articles on statistics when there are already many excellent resources? There are strong introductions at every level, mathematical, applied, and software-focused, and many are freely available. Some emphasize theory and proofs; others emphasize implementation in R, Python, Stata, or SQL; still others are field-specific (biostatistics, engineering, economics, business, and more).\nThis collection exists for two reasons.\nFirst, the skill mix in quantitative work is changing. AI coding assistants can already generate a large share of routine statistical code quickly, and this capability is improving fast. As a result, the comparative advantage of researchers and data analysts is shifting away from writing every line by hand and toward:\nIn other words, implementation still matters, but interpretation, judgment, and communication matter even more.\nSecond, I wanted to build the resource I wish I had during my PhD. Traditional textbooks taught the formal material well, but many practical research decisions were left implicit: Which method is appropriate here? What trade-offs am I making? What can go wrong in real data? How should results be interpreted and communicated?\nThese articles aim to close that gap. They connect core statistical ideas to real research workflows, with concise explanations, worked examples, and decision-oriented guidance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Beta to Meaning</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "2  Preface",
    "section": "",
    "text": "2.1 Prerequisites\nThis preface explains why the book exists and how it is organized.\nHigh school level math. It is helpful if you have some experience with R so you understand the code snippets, but it is not necessary to understand the issues that I try to address in this book.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#how-to-use-this-book",
    "href": "preface.html#how-to-use-this-book",
    "title": "2  Preface",
    "section": "2.2 How to use this book",
    "text": "2.2 How to use this book\nAll data used in this book is simulated. Even though the goal of the book is not to demonstrate how to use a statistical software, code snippets are included to illustrate how to implement the methods discussed in the book. The code is written in R, but the concepts can be applied in other programming languages as well. The code snippets are meant to be illustrative and are not intended to be comprehensive or optimized for performance. Readers are encouraged to experiment with the code and adapt it to their own needs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html",
    "href": "chapters/01-introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Why this book\nWhy another book on statistics? There are already many excellent introductions, both technical and non-technical, and many are freely available online. Some focus on the mathematical foundations of statistics, while others provide practical guidance on applying statistical methods across contexts or implementing them in software such as R, Python, or Stata. Still others are tailored to specific disciplines and industries such as health and biostatistics, engineering, economics, and business. So why write another one?\nThere are two reasons why I decided to write this book. First, coding and implementing statistics into software is a skill that is likely to become less important with the arrival of AI-coding assistants. Most agents such as Claude Code or Codex can write better code in R, SQL, Python or other languages than most humans, and they can do it much faster. While some coding skills will still be necessary to understand how to use these tools as well as to debug and adjust code for specific use cases, the need to write code from scratch will likely diminish over time. What will become more important, however, is the ability to clearly communicate what a user wants an agent to do and how to understand and interpret the results that an agent produces.\nThis brings me to the second reason for writing this book. The book is also going to be the book that I hoped I had when I was learning statistics and doing research as a PhD student. While there were many great books on statistics already available, I found that there were a lot of open questions when doing quantitative research that were not covered in these books.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01-introduction.html#roadmap",
    "href": "chapters/01-introduction.html#roadmap",
    "title": "2  Introduction",
    "section": "2.2 Roadmap",
    "text": "2.2 Roadmap\nOutline the main themes and how the chapters connect.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html",
    "href": "chapters/02-chapter-one.html",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "",
    "text": "4.1 Binary Predictor Variables\nDisclaimer: All data used in this chapter is simulated and does not reflect real-world data. The purpose of this chapter is to illustrate the concept of categorical and compositional predictors in regression analysis, not to make any claims about the relationship between gender, party identification, and political donations or party shares and social spending.\nI already discussed how to interpret coefficients in a regression model. What I have not discussed is that the interpretation of a coefficient as the expected change in the response variable for a one-unit increase in the predictor variable (holding all other predictors constant) is most direct when a one-unit change is substantively meaningful. For binary and categorical predictors, coefficients are instead interpreted as differences between categories defined by the coding scheme. This has consequences for how we specify models and interpret results as I will show in the following.\nSuppose I study the relationship between gender and political donations. I have a binary variable gender coded 0 for men and 1 for women and donationAmount in US dollars. To understand how categorical variables work in regression analysis, I start by computing the average donation amount for men and women:\nmean_donation &lt;- aggregate(donationAmount ~ gender, data = data, FUN = mean)\nmean_donation$gender &lt;- c(\"Men\", \"Women\")\ncolnames(mean_donation) &lt;- c(\"Gender\", \"Mean Donation Amount\")\nknitr::kable(mean_donation, digits = 2)\n\n\n\n\nGender\nMean Donation Amount\n\n\n\n\nMen\n98.75\n\n\nWomen\n148.22\nAs the table shows, men donate on average 98.75 dollars, while women donate on average 148.22 dollars. With that in mind, I estimate a regression model with donationAmount as the response variable and gender as the predictor variable. Mathematically, the model can be written as follows:\n\\[donationAmount_i = \\alpha_0 + \\beta_1 gender_i + \\epsilon_i\\]\nIn R, I estimate the model using the lm() function:\nlm(donationAmount ~ factor(gender), data = data)\n\n\nCall:\nlm(formula = donationAmount ~ factor(gender), data = data)\n\nCoefficients:\n    (Intercept)  factor(gender)1  \n          98.75            49.46\nI start with the interpretation of the results before digging deeper. The coefficient for gender is about 50. In the case of a binary variable, the interpretation is straightforward: The coefficient compares the coded group (1) to the reference group (0). More precisely, the coefficient of about 50 for gender means that women donate on average 50 dollars more than men.\nComparing the regression output to the means table, I can highlight two points. First, the coefficient of the intercept is about 98.75, which is the average donation amount for men. Put differently, the intercept equals the mean of the reference category of men in this specification because, more generally, the intercept is the expected outcome when all predictors are zero and factors are at their baseline levels. This can be expressed mathematically:\n\\[E(donationAmount \\mid \\text{men}) = \\alpha_0\\]\nThis is why, when the model includes an intercept, one category must be omitted; otherwise the full set of dummies is perfectly collinear with the intercept (the dummy-variable trap). That means that if I included binary variables for both men and women in the model, the dummy variables would be perfectly collinear with the intercept and the model would not be estimable.\nSecond, the coefficient for gender is about 50, which is the difference in mean donation amounts between men and women. Thus, the coefficient for gender does not give us the average donation amount for women, but rather the difference in average donation amounts between women and men. In turn, this means that the mean donation amount of women is the sum of the coefficient of the intercept and the coefficient for gender:\n\\[E(donationAmount \\mid \\text{women}) = \\alpha_0 + \\beta_1\\]\nLastly, the substantive interpretation of the results does not change if I alter the reference category. With pure releveling (changing only which category is omitted), fitted values and model fit stay the same, but coefficient labels and interpretations change. To illustrate this, I can relevel the gender variable so that women are the reference category and are omitted from the model:\nlm(donationAmount ~ relevel(factor(gender), ref = \"1\"), data = data)\n\n\nCall:\nlm(formula = donationAmount ~ relevel(factor(gender), ref = \"1\"), \n    data = data)\n\nCoefficients:\n                        (Intercept)  relevel(factor(gender), ref = \"1\")0  \n                             148.22                               -49.46\nAs shown, the coefficient for intercept is now about 148, which is equal to the average donation amount for women according to the means table. The coefficient for gender (level 0) now displays the difference in donation amount of men compared to women. More precisely, it is now about -50, meaning that men donate on average 50 dollars less than women. Again, summing the coefficient of the intercept and the coefficient for gender gives the average donation amount for men. Thus, while the coefficients of intercept and gender have changed, the substantive interpretation of the results has not. The choice of reference category does not affect the substantive interpretation of the results, but it does affect how the coefficients are read.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html",
    "href": "chapters/03-chapter-two.html",
    "title": "4  Context Matters: A Guide to Interaction Terms",
    "section": "",
    "text": "4.1 Interaction Terms as Context\nDisclaimer: All data used in this chapter is simulated and does not reflect real-world data. The purpose of this chapter is to illustrate the concept of interaction terms in linear regression models, not to make any claims about the relationship between age, income, and ideological placement.\nResearchers often want to know whether relationships differ across contexts. We use ideological self-placement as the running example. More specifically, I want to know whether age affects left-right self-placement. In addition, I assume that income affects ideological leanings as well, meaning that voters at the lower end of the income distribution are typically more left than voters on the higher end of the income distribution. So, how can interaction terms help us understand the relationship between age, income and ideological placement? Does the relationship between age and ideological placement differ across income levels? If so, how can we model this relationship?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#what-is-an-interaction-term",
    "href": "chapters/03-chapter-two.html#what-is-an-interaction-term",
    "title": "4  Context Matters: A Guide to Interaction Terms",
    "section": "4.2 What is an Interaction Term?",
    "text": "4.2 What is an Interaction Term?\nIn this example, income is measured in tens of thousands, and leftRight is a 0-10 self-placement scale where higher values indicate more right-leaning views.\nWe established in the previous chapter that a coefficient in a regression model represents the average effect of a predictor variable on the response variable, holding all other variables constant. However, this assumes that the effect of the predictor is the same across all contexts. Using a linear regression model with a continuous response variable leftRight and a predictor variable age, we can illustrate this point. Mathematically, our model without an interaction term looks as follows:\n\\[\nleftRight_i = \\alpha + \\beta_1 age_i + \\epsilon_i\n\\tag{4.1}\\]\nEquation 4.1 shows that the marginal effect of a predictor variable in a linear regression model is represented by the coefficient \\(\\beta_1\\). To understand interactions, it is helpful to know that the marginal effect of a predictor can be read directly from the equation because it is the partial derivative of the response variable with respect to the predictor variable. In our simple regression model, the marginal effect of age is \\(\\beta_1\\), which means that for every one-unit increase in age, the expected change in leftRight is \\(\\beta_1\\) units. Mathematically, the partial derivative of Equation 4.1 can be expressed as: \\[\n\\frac{\\partial leftRight_i}{\\partial age_i} = \\beta_1\n\\tag{4.2}\\]\nIn R, this model can be estimated using the lm() function as follows:\n\nsummary(\n  lm(leftRight ~ age, data = data)\n)\n\n\nCall:\nlm(formula = leftRight ~ age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5059 -0.8757 -0.0084  0.9727  3.0922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.697214   0.118956   31.08   &lt;2e-16 ***\nage         0.045269   0.002289   19.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.289 on 998 degrees of freedom\nMultiple R-squared:  0.2816,    Adjusted R-squared:  0.2809 \nF-statistic: 391.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nSo, why is this important to understand interaction terms? An interaction term is essentially a multiplicative combination of two predictor variables. When we include an interaction term in our regression model, we simply add two predictors and their product to the equation. Let’s assume that we want to know whether the effect of age on leftRight depends on the income of a respondent. Our regression model with the interaction term would look like this: \\[\nleftRight_i = \\alpha + \\beta_1 age_i + \\beta_2 income_i + \\beta_3 (age_i \\times income_i) + \\epsilon_i\n\\tag{4.3}\\]\nIf we take the partial derivative of Equation 4.3 with respect to age, we get: \\[\n\\frac{\\partial leftRight_i}{\\partial age_i} = \\beta_1 + \\beta_3 income_i\n\\tag{4.4}\\]\nAs Equation 4.4 shows, the effect of age on leftRight is no longer constant but depends on the value of income. The coefficient \\(\\beta_3\\) represents the change in the effect of age for each one-unit increase in income. If \\(\\beta_3\\) is positive, it means that the effect of age on leftRight increases as income increases. Conversely, if \\(\\beta_3\\) is negative, it means that the effect of age on leftRight decreases as income increases.\nIn R, this would look like this:\n\nsummary(\n  lm(leftRight ~ age * income, data = data)\n)\n\n\nCall:\nlm(formula = leftRight ~ age * income, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.45589 -0.55774  0.02019  0.56182  2.66821 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.4255133  0.2030559  11.945  &lt; 2e-16 ***\nage         0.0224525  0.0038078   5.896 5.08e-09 ***\nincome      0.1625065  0.0264208   6.151 1.12e-09 ***\nage:income  0.0037022  0.0005002   7.401 2.86e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8063 on 996 degrees of freedom\nMultiple R-squared:  0.7196,    Adjusted R-squared:  0.7187 \nF-statistic: 851.9 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n(R adds the constitutive terms automatically when you use the * operator between two variables in the formula.)\nEven though it is more informative to plot the interaction to understand how the effect of age changes across different income levels, we can also look at the coefficients to get a sense of the interaction. First, we can see that the coefficient for age is positive. However, the coefficient for age gives us only the effect of age when income is zero, which is not a meaningful value in this context. A solution here would be to center the income variable around its mean before including it in the regression model. This way, the coefficient for age would represent the effect of age at the average income level.\nSecond, the coefficient for the interaction term age:income is positive, which indicates that the effect of age on leftRight increases as income increases. This means that age is more strongly associated with rightward placement among higher-income respondents than among lower-income respondents. Therefore, the table alone already gives us a pretty good idea of the interaction, but it is often more intuitive to visualize it:\n\n# Fit the model with interaction\nmodel &lt;- lm(leftRight ~ age * income, data = data)\n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model)\nvcov_matrix &lt;- vcov(model)\n\nincome_seq &lt;- seq(2, 12, length.out = 100)\n\neffect &lt;- coefs[2] + coefs[4] * income_seq\n\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           income_seq^2 * vcov_matrix[4, 4] + \n           2 * income_seq * vcov_matrix[2, 4])\n\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(income_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n  xlab = \"Income (tens of thousands)\",\n  ylab = \"Effect of Age on Left-Right Placement\",\n  main = \"Marginal Effect of Age by Income\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(income_seq, rev(income_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.2, 0.2, 0.8, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(income_seq, effect, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nTo plot the marginal effect of age on leftRight across different income levels, along with 95% confidence intervals, we use the coefficients and variance-covariance matrix from the fitted model. To compute variance for the confidence intervals, we apply the delta method:\n\\[Var(\\beta_1 + \\beta_3 \\cdot income) = Var(\\beta_1) + income^2 \\cdot Var(\\beta_3) + 2 \\cdot income \\cdot Cov(\\beta_1, \\beta_3)\\]\nThe plot shows that the estimated effect of age is positive across the observed income range. This visualization gives us a much clearer picture of how the effect of age changes across income levels compared to a regression table. While age increases rightward placement for all income levels, the effect is much stronger for higher-income respondents than for lower-income respondents.\nAn alternative way to visualize the interaction is to plot predicted values of leftRight across age for different income levels. To do so, we use the 10%, 50%, and 90% quantiles of the income variable to represent low, medium, and high income levels, respectively. We then plot the predicted leftRight values across age for these three income levels, along with confidence intervals.\n\nincome_levels &lt;- as.numeric(quantile(data$income, probs = c(0.1, 0.5, 0.9)))\nincome_labels &lt;- paste0(c(\"P10 = \", \"P50 = \", \"P90 = \"), sprintf(\"%.1f\", income_levels))\nage_grid &lt;- seq(18, 80, length.out = 100)\n\npred_grid &lt;- expand.grid(\n  age = age_grid,\n  income = income_levels\n)\n\npred_ci &lt;- predict(model, newdata = pred_grid, interval = \"confidence\")\npred_grid$leftRight_hat &lt;- pred_ci[, \"fit\"]\npred_grid$ci_lower &lt;- pred_ci[, \"lwr\"]\npred_grid$ci_upper &lt;- pred_ci[, \"upr\"]\n\nplot(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[1]],\n     type = \"l\",\n     lwd = 2,\n     ylim = range(pred_grid$ci_lower, pred_grid$ci_upper),\n     xlab = \"Age\",\n     ylab = \"Predicted Left-Right Placement\",\n    main = \"Predicted Left-Right by Age\\n(at Income P10, P50, and P90)\")\n\npolygon(c(age_grid, rev(age_grid)),\n        c(pred_grid$ci_upper[pred_grid$income == income_levels[1]],\n          rev(pred_grid$ci_lower[pred_grid$income == income_levels[1]])),\n        col = rgb(0, 0, 0, 0.15),\n        border = NA)\n\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[2]], lwd = 2, col = \"blue\")\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[3]], lwd = 2, col = \"red\")\n\npolygon(c(age_grid, rev(age_grid)),\n        c(pred_grid$ci_upper[pred_grid$income == income_levels[2]],\n          rev(pred_grid$ci_lower[pred_grid$income == income_levels[2]])),\n        col = rgb(0, 0, 1, 0.15),\n        border = NA)\n\npolygon(c(age_grid, rev(age_grid)),\n        c(pred_grid$ci_upper[pred_grid$income == income_levels[3]],\n          rev(pred_grid$ci_lower[pred_grid$income == income_levels[3]])),\n        col = rgb(1, 0, 0, 0.15),\n        border = NA)\n\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[1]], lwd = 2, col = \"black\")\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[2]], lwd = 2, col = \"blue\")\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[3]], lwd = 2, col = \"red\")\n\nlegend(\"topleft\",\n  legend = income_labels,\n       lwd = 2,\n       col = c(\"black\", \"blue\", \"red\"),\n       bty = \"n\")\n\n\n\n\n\n\n\n\nThe plot shows that the predicted leftRight values increase with age for all three income levels, but the slope is steeper for higher-income respondents. This visualization provides an alternative way to understand how the relationship between age and leftRight changes across different income levels, compared to the previous plot of marginal effects. Which visualization is more informative depends on the research question and the audience.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#a-brief-digression-polynomial-regression",
    "href": "chapters/03-chapter-two.html#a-brief-digression-polynomial-regression",
    "title": "4  Context Matters: A Guide to Interaction Terms",
    "section": "4.3 A Brief Digression: Polynomial Regression",
    "text": "4.3 A Brief Digression: Polynomial Regression\nWe now extend the interaction idea to nonlinearity. In an interaction, the slope of one variable changes with another variable (x × z). With polynomial terms, the slope of a variable changes with its own level by including powers like x² and x³. Although both are multiplicative in form, polynomial terms are conventionally treated as predictor transformations (basis expansions), not as interaction terms. Using age as an example, our regression model with a cubic term would look like this: \\[\nleftRight_i = \\alpha + \\beta_1 age_i + \\beta_2 age_i^2 + \\beta_3 age_i^3 + \\epsilon_i\n\\tag{4.5}\\]\nThe partial derivative of Equation 4.5 with respect to age is: \\[\n\\frac{\\partial leftRight_i}{\\partial age_i} = \\beta_1 + 2 \\beta_2 age_i + 3 \\beta_3 age_i^2\n\\tag{4.6}\\]\nIn R, a polynomial regression can be estimated by either using the poly() function (or including the polynomial terms manually) as follows:\n\nsummary(\n  lm(leftRight ~ poly(age, degree = 3, raw = TRUE), data = data2)\n)\n\n\nCall:\nlm(formula = leftRight ~ poly(age, degree = 3, raw = TRUE), data = data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25298 -0.55461  0.01145  0.54104  2.61350 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         3.510e+00  5.398e-01   6.502 1.25e-10 ***\npoly(age, degree = 3, raw = TRUE)1  1.861e-01  3.825e-02   4.865 1.33e-06 ***\npoly(age, degree = 3, raw = TRUE)2 -3.574e-03  8.315e-04  -4.298 1.89e-05 ***\npoly(age, degree = 3, raw = TRUE)3  2.323e-05  5.631e-06   4.126 4.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8007 on 996 degrees of freedom\nMultiple R-squared:  0.1354,    Adjusted R-squared:  0.1328 \nF-statistic:    52 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the plot of the marginal effect of age on leftRight would look like this:\n\n# Fit the polynomial regression model with raw polynomials\nmodel_poly &lt;- lm(leftRight ~ age + I(age^2) + I(age^3), data = data2) \n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model_poly)\nvcov_matrix &lt;- vcov(model_poly)\n\n# Create age sequence for prediction\nage_seq &lt;- seq(18, 80, length.out = 100)\n\n# Calculate marginal effect: β₁ + 2*β₂*age + 3*β₃*age²\neffect &lt;- coefs[2] + 2 * coefs[3] * age_seq + 3 * coefs[4] * age_seq^2\n\n# Delta method for standard error:\n# For f(β) = β₁ + 2*β₂*age + 3*β₃*age², gradient is [1, 2*age, 3*age²]\n# SE = sqrt(gradient' * Vcov * gradient)\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           4 * age_seq^2 * vcov_matrix[3, 3] + \n           9 * age_seq^4 * vcov_matrix[4, 4] +\n           4 * age_seq * vcov_matrix[2, 3] +\n           6 * age_seq^2 * vcov_matrix[2, 4] +\n           12 * age_seq^3 * vcov_matrix[3, 4])\n\n# Calculate 95% confidence intervals\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(age_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n     xlab = \"Age\",\n    ylab = \"Marginal Effect of Age on Left-Right Placement\",\n     main = \"Marginal Effect of Age\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(age_seq, rev(age_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.8, 0.2, 0.2, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(age_seq, effect, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\nThe plot shows that the marginal effect of age on leftRight is positive but decreasing for younger respondents, becomes not statistically distinguishable from zero for respondents between mid-40s and mid-50s, and becomes positive and significant again for older respondents. This U-shaped pattern illustrates how polynomial regression can capture non-linear relationships between a predictor variable and a response variable. In this example, the relationship between age and leftRight follows a nonlinear pattern with changing slope that can be captured by including polynomial terms in the regression model.\nOf course, other functional forms could be considered to capture different types of non-linear relationships. But the main point is that this example illustrates how polynomial regression can capture non-linear relationships between a predictor variable and a response variable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html#shares",
    "href": "chapters/02-chapter-one.html#shares",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "4.3 Shares",
    "text": "4.3 Shares\nAnother case that is subject to the same problem of reference categories but is often underemphasized in the literature is the case of shares. For instance, researchers are often interested in the effect of the share of the population living in urban areas, the share of the agricultural or industrial workforce, the share of ethnic or religious groups, or the share of the population with a certain level of education on some outcome (to name but a few examples). In all these cases, the share variable is bounded between 0 and 1 and thus has an implicit complement (e.g., rural share = 1 − urban share). For instance, if I have a variable that measures the share of the population living in urban areas, then a value of 0 means everyone is in the complement category, and a value of 1 means no one is in the complement category. This means that the interpretation of the coefficients for share variables is subject to the same problems as before and depends on the number of categories and on which share(s) are omitted from the model.\nTo illustrate the problem, consider party shares in government. Suppose I observe the share of cabinet seats held by left, center, and right parties. Each share is between 0 and 1 and the three shares sum to 1. Because the shares sum to 1, these relationships are always interpreted as shifts in composition (increasing one share necessarily decreases at least one other share). I also observe social spending per capita which serves as the response variable.\nBecause the shares sum to 1, one share must be omitted when I include an intercept. If I omit the right share, the model can be written as:\n\\[\nSocialSpendingpc_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\nHowever, there are a number of different ways to specify the model. Since I can no longer display the mean of the response variable by group (as I did above), I establish a baseline by estimating a model with all party groups and no intercept. After that, I can drop the right share which is the reference category in the first model to demonstrate how both models are connected.\n\nmodel_drop_right &lt;- lm(social_spend_pc ~ left + center, data = data_party_share)\nmodel_no_intercept &lt;- lm(social_spend_pc ~ left + center + right - 1, data = data_party_share)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_drop_right &lt;- coef(summary(model_drop_right))\ncs_no_intercept &lt;- coef(summary(model_no_intercept))\n\nterms &lt;- c(\"(Intercept)\", \"left\", \"center\", \"right\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `No_Intercept` = c(\n    \"—\",\n    fmt_coef(cs_no_intercept, \"left\"),\n    fmt_coef(cs_no_intercept, \"center\"),\n    fmt_coef(cs_no_intercept, \"right\")\n  ),\n  `No_Right` = c(\n    fmt_coef(cs_drop_right, \"(Intercept)\"),\n    fmt_coef(cs_drop_right, \"left\"),\n    fmt_coef(cs_drop_right, \"center\"),\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party-share regressions with different baselines\"\n)\n\n\nParty-share regressions with different baselines\n\n\nTerm\nNo_Intercept\nNo_Right\n\n\n\n\n(Intercept)\n—\n517.22 (120.67)\n\n\nleft\n6987.47 (114.32)\n6470.26 (199.61)\n\n\ncenter\n1460.77 (113.01)\n943.56 (194.27)\n\n\nright\n517.22 (120.67)\nref\n\n\n\n\n\nThe no-intercept model displays the expected level of social spending per capita if a given party held all cabinet seats (i.e., its share equals 1 and the other shares equal 0). For example, the coefficient for left of about 7000 means that if a government were composed entirely of left parties, I would expect social spending per capita to be almost 7000. Similarly, the coefficients for center and right show the expected spending levels under hypothetical all-center or all-right governments. This specification serves as a diagnostic baseline because it shows the direction and magnitude of each party share’s association with social spending.\nIn the second model with an intercept, the right shares variable is absorbed into the intercept, and each remaining coefficient becomes a contrast against the omitted share rather than a standalone predicted level. The coefficient for intercept is now the expected level of social spending per capita if a government were composed entirely of right parties (i.e., right share equals 1 and the other shares equal 0). The coefficients for left and center are now interpreted as the expected change in social spending per capita for a one-unit (i.e., 100 percentage point) increase in the left (center) share and a simultaneous one-unit decrease in the right share, holding the center (left) share variable constant. Thus, the relationship between the left share and social spending can be mathematically expressed as follows:\n\\[E(SocialSpendingpc \\mid left=1, center=0, right=0) = \\alpha_0 + \\beta_1\\]\nAdding the coefficient for the intercept and the coefficient for left in the second model reproduces the coefficient for left in the no-intercept model. This means that the expected level of social spending per capita if a government were composed entirely of left parties is the sum of the coefficient for intercept and the coefficient for left in the second model, which matches the coefficient for left in the no-intercept model. The same logic applies to the center share.\nThe example illustrates that models with share variables are subject to the same issues of reference categories as models with categorical predictors. The interpretation of the coefficients depends on which share(s) are omitted from the model, and the choice of reference category can change the substantive interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html#predictor-variables-with-more-than-two-categories",
    "href": "chapters/02-chapter-one.html#predictor-variables-with-more-than-two-categories",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "4.2 Predictor Variables with More than Two Categories",
    "text": "4.2 Predictor Variables with More than Two Categories\nThe logic becomes a bit more complex when I have more than two categories. For instance, suppose I have another categorical variable called party identification with four possible groups: Left, Center, Right, and Other. To illustrate the link between reference categories and group means, I display the average donation amount for each party identification group in the table below. In addition, I add a row for the average donation amount for the combined Right and Other group, which will help me to illustrate what changes when I omit multiple categories (instead of one category) in the regression model.\n\nmean_party &lt;- aggregate(donationAmount ~ party, data = data.frame(\n  party = party,\n  donationAmount = donationAmount\n), FUN = mean)\nmean_ro &lt;- mean(donationAmount[party %in% c(\"Right\", \"Other\")])\nmean_party &lt;- rbind(\n  mean_party,\n  data.frame(party = \"Right + Other\", donationAmount = mean_ro)\n)\ncolnames(mean_party) &lt;- c(\"Party\", \"Mean Donation Amount\")\nknitr::kable(mean_party, digits = 2)\n\n\n\n\nParty\nMean Donation Amount\n\n\n\n\nCenter\n115.45\n\n\nLeft\n140.90\n\n\nOther\n101.14\n\n\nRight\n124.15\n\n\nRight + Other\n117.80\n\n\n\n\n\nTo estimate the relationship of a respondent’s party identification and donationAmount in a linear regression model, I need to choose one of the categories as the reference category. To illustrate what happens when I omit more than one category, I specify one regression model with Other as the reference category and another regression model with Right and Other jointly omitted (a pooled reference category). Mathematically, the first model can be written as follows:\n\\[\ndonationAmount_i  = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\beta_3 right_i + \\epsilon_i\n\\]\nWith Right and Other pooled into a single reference category, the second model becomes:\n\\[\ndonationAmount_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\n\n# Leave out \"Other\" as the reference category\nmodel_other_ref &lt;- lm(donationAmount ~ left + center + right, data = data_party)\n\n# Leave out \"Right\" and \"Other\" as reference categories\nmodel_ro_ref &lt;- lm(donationAmount ~ left + center, data = data_party)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_other &lt;- coef(summary(model_other_ref))\ncs_ro_ref &lt;- coef(summary(model_ro_ref))\n\nterms &lt;- c(\"(Intercept)\", \"Left\", \"Center\", \"Right\", \"Other\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `Ref:Other` = c(\n    fmt_coef(cs_other, \"(Intercept)\"),\n    fmt_coef(cs_other, \"left\"),\n    fmt_coef(cs_other, \"center\"),\n    fmt_coef(cs_other, \"right\"),\n    \"ref\"\n  ),\n  `Ref:Right_Other` = c(\n    fmt_coef(cs_ro_ref, \"(Intercept)\"),\n    fmt_coef(cs_ro_ref, \"left\"),\n    fmt_coef(cs_ro_ref, \"center\"),\n    \"ref\",\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party effects with different reference categories\"\n)\n\n\nParty effects with different reference categories\n\n\nTerm\nRef.Other\nRef.Right_Other\n\n\n\n\n(Intercept)\n101.14 (3.32)\n117.80 (1.80)\n\n\nLeft\n39.77 (3.83)\n23.11 (2.65)\n\n\nCenter\n14.31 (3.75)\n-2.35 (2.54)\n\n\nRight\n23.02 (3.91)\nref\n\n\nOther\nref\nref\n\n\n\n\n\nThe first column of the table reports estimates with Other as the reference category, so the coefficients for Left, Center, and Right are each interpreted relative to Other. This means that the coefficient of the intercept is the average donation amount for respondents identifying as Other, and the coefficients for Left, Center, and Right show how much more (or less) respondents in these groups donate on average compared to respondents in Other. As shown before, the results can be easily cross-checked with the means table. While the coefficient for the intercept in the regression is equal to the mean donation amount for Other respondents in the means table, the coefficients for Left, Center, and Right have to be added to the intercept to get the mean donation amount for these groups, which matches the values in the means table as well. For example, the average donation amount for Left respondents in the first column can be deduced as follows: \\[E(donationAmount \\mid \\text{Left}) = \\alpha_0 + \\beta_1 \\]\nwhich is about 101.14 + 39.77 = 140.91 dollars, which matches roughly the average donation amount for Left respondents in the means table (aside from differences in rounding). The same logic can be applied to the coefficients for Center and Right.\nIn the second column, however, I change the specification by omitting Right as well, which means Right and Other are pooled into a single reference category. Thus, this is not merely releveling; it is a different model specification because it compares Left and Center to Right and Other. Coefficients therefore change meaning. While the interpretation is still straightforward as before, the estimand is different from the first model.\nMore precisely, the coefficient for the intercept in the second column is now the average donation amount for respondents identifying as Right or Other. This can be confirmed by comparing the coefficient of the intercept with the row for Right or Other in the means table. This also means that the coefficients for Left and Center are now interpreted as the difference in average donation amounts between respondents identifying as either Left or Center on one hand and respondents identifying as Right or Other on the other hand. And since the coefficient of the intercept increased by almost 17 dollars compared to the first column, the coefficients for Left and Center decreased accordingly. For instance, the coefficient for Center is now negative, which reflects the fact that the average donation amount for respondents identifying as Center is 2.35 dollars less than the average donation amount for respondents identifying as Right or Other as the means table shows.\nThe main takeaway is that with pure releveling, fitted values and model fit do not change; only coefficient labels and comparisons do. With pooling reference categories, you change the comparison group and therefore the estimand and the substantive conclusions that you can draw.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/02-chapter-one.html#compositional-predictors",
    "href": "chapters/02-chapter-one.html#compositional-predictors",
    "title": "4  Categorical and Compositional Predictors: A Guide",
    "section": "4.3 Compositional Predictors",
    "text": "4.3 Compositional Predictors\nAnother case that faces the same reference category problem, yet is often underemphasized in the literature, is compositional predictors, where values are interpretable only relative to one another and are constrained to sum to a constant, typically 1 or 100%. For instance, researchers are often interested in the effect of the share of the population living in urban areas, the share of the agricultural or industrial workforce, the share of ethnic or religious groups, or the share of the population with a certain level of education on some outcome (to name but a few examples). In all these cases, the share variable is bounded between 0 and 1 and thus has an implicit complement (e.g., rural share = 1 − urban share). For instance, if I have a variable that measures the share of the population living in urban areas, then a value of 0 means everyone is in the complement category, and a value of 1 means no one is in the complement category. This means that the interpretation of the coefficients for share variables is subject to the same problems as before and depends on the number of categories and on which share(s) are omitted from the model.\nTo illustrate the problem, consider party shares in government. Suppose I observe the share of cabinet seats held by left, center, and right parties. Each share is between 0 and 1 and the three shares sum to 1. Because the shares sum to 1, these relationships are always interpreted as shifts in composition (increasing one share necessarily decreases at least one other share). I also observe social spending per capita which serves as the response variable.\nBecause the shares sum to 1, one share must be omitted when I include an intercept. If I omit the right share, the model can be written as:\n\\[\nSocialSpendingpc_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\nHowever, there are a number of different ways to specify the model. Since I can no longer display the mean of the response variable by group (as I did above), I establish a baseline by estimating a model with all party groups and no intercept. After that, I can drop the right share which is the reference category in the first model to demonstrate how both models are connected.\n\nmodel_drop_right &lt;- lm(social_spend_pc ~ left + center, data = data_party_share)\nmodel_no_intercept &lt;- lm(social_spend_pc ~ left + center + right - 1, data = data_party_share)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_drop_right &lt;- coef(summary(model_drop_right))\ncs_no_intercept &lt;- coef(summary(model_no_intercept))\n\nterms &lt;- c(\"(Intercept)\", \"left\", \"center\", \"right\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `No_Intercept` = c(\n    \"—\",\n    fmt_coef(cs_no_intercept, \"left\"),\n    fmt_coef(cs_no_intercept, \"center\"),\n    fmt_coef(cs_no_intercept, \"right\")\n  ),\n  `No_Right` = c(\n    fmt_coef(cs_drop_right, \"(Intercept)\"),\n    fmt_coef(cs_drop_right, \"left\"),\n    fmt_coef(cs_drop_right, \"center\"),\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party-share regressions with different baselines\"\n)\n\n\nParty-share regressions with different baselines\n\n\nTerm\nNo_Intercept\nNo_Right\n\n\n\n\n(Intercept)\n—\n517.22 (120.67)\n\n\nleft\n6987.47 (114.32)\n6470.26 (199.61)\n\n\ncenter\n1460.77 (113.01)\n943.56 (194.27)\n\n\nright\n517.22 (120.67)\nref\n\n\n\n\n\nThe no-intercept model displays the expected level of social spending per capita if a given party held all cabinet seats (i.e., its share equals 1 and the other shares equal 0). For example, the coefficient for left of about 7000 means that if a government were composed entirely of left parties, I would expect social spending per capita to be almost 7000. Similarly, the coefficients for center and right show the expected spending levels under hypothetical all-center or all-right governments. This specification serves as a diagnostic baseline because it shows the direction and magnitude of each party share’s association with social spending.\nIn the second model with an intercept, the right shares variable is absorbed into the intercept, and each remaining coefficient becomes a contrast against the omitted share rather than a standalone predicted level. The coefficient for intercept is now the expected level of social spending per capita if a government were composed entirely of right parties (i.e., right share equals 1 and the other shares equal 0). The coefficients for left and center are now interpreted as the expected change in social spending per capita for a one-unit (i.e., 100 percentage point) increase in the left (center) share and a simultaneous one-unit decrease in the right share, holding the center (left) share variable constant. Thus, the relationship between the left share and social spending can be mathematically expressed as follows:\n\\[E(SocialSpendingpc \\mid left=1, center=0, right=0) = \\alpha_0 + \\beta_1\\]\nAdding the coefficient for the intercept and the coefficient for left in the second model reproduces the coefficient for left in the no-intercept model. This means that the expected level of social spending per capita if a government were composed entirely of left parties is the sum of the coefficient for intercept and the coefficient for left in the second model, which matches the coefficient for left in the no-intercept model. The same logic applies to the center share.\nThe example illustrates that models with share variables are subject to the same issues of reference categories as models with categorical predictors. The interpretation of the coefficients depends on which share(s) are omitted from the model, and the choice of reference category can change the substantive interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/03-chapter-two.html#interaction-terms-as-context",
    "href": "chapters/03-chapter-two.html#interaction-terms-as-context",
    "title": "4  Context Matters: A Guide to Interaction Terms",
    "section": "",
    "text": "“If You Are Not a Liberal at 25, You Have No Heart. If You Are Not a Conservative at 35 You Have No Brain”\n— Quote Investigator",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/Categorical_Compositional.html",
    "href": "chapters/Categorical_Compositional.html",
    "title": "2  Categorical and Compositional Predictors: A Guide",
    "section": "",
    "text": "2.1 Binary Predictor Variables\nDisclaimer: All data used in this chapter is simulated and does not reflect real-world data. The purpose of this chapter is to illustrate the concept of categorical and compositional predictors in regression analysis, not to make any claims about the relationship between gender, party identification, and political donations or party shares and social spending.\nI already discussed how to interpret coefficients in a regression model. What I have not discussed is that the interpretation of a coefficient as the expected change in the response variable for a one-unit increase in the predictor variable (holding all other predictors constant) is most direct when a one-unit change is substantively meaningful. For binary and categorical predictors, coefficients are instead interpreted as differences between categories defined by the coding scheme. This has consequences for how we specify models and interpret results as I will show in the following.\nSuppose I study the relationship between gender and political donations. I have a binary variable gender coded 0 for men and 1 for women and donationAmount in US dollars. To understand how categorical variables work in regression analysis, I start by computing the average donation amount for men and women:\nmean_donation &lt;- aggregate(donationAmount ~ gender, data = data, FUN = mean)\nmean_donation$gender &lt;- c(\"Men\", \"Women\")\ncolnames(mean_donation) &lt;- c(\"Gender\", \"Mean Donation Amount\")\nknitr::kable(mean_donation, digits = 2)\n\n\n\n\nGender\nMean Donation Amount\n\n\n\n\nMen\n98.75\n\n\nWomen\n148.22\nAs the table shows, men donate on average 98.75 dollars, while women donate on average 148.22 dollars. With that in mind, I estimate a regression model with donationAmount as the response variable and gender as the predictor variable. Mathematically, the model can be written as follows:\n\\[donationAmount_i = \\alpha_0 + \\beta_1 gender_i + \\epsilon_i\\]\nIn R, I estimate the model using the lm() function:\nlm(donationAmount ~ factor(gender), data = data)\n\n\nCall:\nlm(formula = donationAmount ~ factor(gender), data = data)\n\nCoefficients:\n    (Intercept)  factor(gender)1  \n          98.75            49.46\nI start with the interpretation of the results before digging deeper. The coefficient for gender is about 50. In the case of a binary variable, the interpretation is straightforward: The coefficient compares the coded group (1) to the reference group (0). More precisely, the coefficient of about 50 for gender means that women donate on average 50 dollars more than men.\nComparing the regression output to the means table, I can highlight two points. First, the coefficient of the intercept is about 98.75, which is the average donation amount for men. Put differently, the intercept equals the mean of the reference category of men in this specification because, more generally, the intercept is the expected outcome when all predictors are zero and factors are at their baseline levels. This can be expressed mathematically:\n\\[E(donationAmount \\mid \\text{men}) = \\alpha_0\\]\nThis is why, when the model includes an intercept, one category must be omitted; otherwise the full set of dummies is perfectly collinear with the intercept (the dummy-variable trap). That means that if I included binary variables for both men and women in the model, the dummy variables would be perfectly collinear with the intercept and the model would not be estimable.\nSecond, the coefficient for gender is about 50, which is the difference in mean donation amounts between men and women. Thus, the coefficient for gender does not give us the average donation amount for women, but rather the difference in average donation amounts between women and men. In turn, this means that the mean donation amount of women is the sum of the coefficient of the intercept and the coefficient for gender:\n\\[E(donationAmount \\mid \\text{women}) = \\alpha_0 + \\beta_1\\]\nLastly, the substantive interpretation of the results does not change if I alter the reference category. With pure releveling (changing only which category is omitted), fitted values and model fit stay the same, but coefficient labels and interpretations change. To illustrate this, I can relevel the gender variable so that women are the reference category and are omitted from the model:\nlm(donationAmount ~ relevel(factor(gender), ref = \"1\"), data = data)\n\n\nCall:\nlm(formula = donationAmount ~ relevel(factor(gender), ref = \"1\"), \n    data = data)\n\nCoefficients:\n                        (Intercept)  relevel(factor(gender), ref = \"1\")0  \n                             148.22                               -49.46\nAs shown, the coefficient for intercept is now about 148, which is equal to the average donation amount for women according to the means table. The coefficient for gender (level 0) now displays the difference in donation amount of men compared to women. More precisely, it is now about -50, meaning that men donate on average 50 dollars less than women. Again, summing the coefficient of the intercept and the coefficient for gender gives the average donation amount for men. Thus, while the coefficients of intercept and gender have changed, the substantive interpretation of the results has not. The choice of reference category does not affect the substantive interpretation of the results, but it does affect how the coefficients are read.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/Categorical_Compositional.html#predictor-variables-with-more-than-two-categories",
    "href": "chapters/Categorical_Compositional.html#predictor-variables-with-more-than-two-categories",
    "title": "2  Categorical and Compositional Predictors: A Guide",
    "section": "2.2 Predictor Variables with More than Two Categories",
    "text": "2.2 Predictor Variables with More than Two Categories\nThe logic becomes a bit more complex when I have more than two categories. For instance, suppose I have another categorical variable called party identification with four possible groups: Left, Center, Right, and Other. To illustrate the link between reference categories and group means, I display the average donation amount for each party identification group in the table below. In addition, I add a row for the average donation amount for the combined Right and Other group, which will help me to illustrate what changes when I omit multiple categories (instead of one category) in the regression model.\n\nmean_party &lt;- aggregate(donationAmount ~ party, data = data.frame(\n  party = party,\n  donationAmount = donationAmount\n), FUN = mean)\nmean_ro &lt;- mean(donationAmount[party %in% c(\"Right\", \"Other\")])\nmean_party &lt;- rbind(\n  mean_party,\n  data.frame(party = \"Right + Other\", donationAmount = mean_ro)\n)\ncolnames(mean_party) &lt;- c(\"Party\", \"Mean Donation Amount\")\nknitr::kable(mean_party, digits = 2)\n\n\n\n\nParty\nMean Donation Amount\n\n\n\n\nCenter\n115.45\n\n\nLeft\n140.90\n\n\nOther\n101.14\n\n\nRight\n124.15\n\n\nRight + Other\n117.80\n\n\n\n\n\nTo estimate the relationship of a respondent’s party identification and donationAmount in a linear regression model, I need to choose one of the categories as the reference category. To illustrate what happens when I omit more than one category, I specify one regression model with Other as the reference category and another regression model with Right and Other jointly omitted (a pooled reference category). Mathematically, the first model can be written as follows:\n\\[\ndonationAmount_i  = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\beta_3 right_i + \\epsilon_i\n\\]\nWith Right and Other pooled into a single reference category, the second model becomes:\n\\[\ndonationAmount_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\n\n# Leave out \"Other\" as the reference category\nmodel_other_ref &lt;- lm(donationAmount ~ left + center + right, data = data_party)\n\n# Leave out \"Right\" and \"Other\" as reference categories\nmodel_ro_ref &lt;- lm(donationAmount ~ left + center, data = data_party)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_other &lt;- coef(summary(model_other_ref))\ncs_ro_ref &lt;- coef(summary(model_ro_ref))\n\nterms &lt;- c(\"(Intercept)\", \"Left\", \"Center\", \"Right\", \"Other\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `Ref:Other` = c(\n    fmt_coef(cs_other, \"(Intercept)\"),\n    fmt_coef(cs_other, \"left\"),\n    fmt_coef(cs_other, \"center\"),\n    fmt_coef(cs_other, \"right\"),\n    \"ref\"\n  ),\n  `Ref:Right_Other` = c(\n    fmt_coef(cs_ro_ref, \"(Intercept)\"),\n    fmt_coef(cs_ro_ref, \"left\"),\n    fmt_coef(cs_ro_ref, \"center\"),\n    \"ref\",\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party effects with different reference categories\"\n)\n\n\nParty effects with different reference categories\n\n\nTerm\nRef.Other\nRef.Right_Other\n\n\n\n\n(Intercept)\n101.14 (3.32)\n117.80 (1.80)\n\n\nLeft\n39.77 (3.83)\n23.11 (2.65)\n\n\nCenter\n14.31 (3.75)\n-2.35 (2.54)\n\n\nRight\n23.02 (3.91)\nref\n\n\nOther\nref\nref\n\n\n\n\n\nThe first column of the table reports estimates with Other as the reference category, so the coefficients for Left, Center, and Right are each interpreted relative to Other. This means that the coefficient of the intercept is the average donation amount for respondents identifying as Other, and the coefficients for Left, Center, and Right show how much more (or less) respondents in these groups donate on average compared to respondents in Other. As shown before, the results can be easily cross-checked with the means table. While the coefficient for the intercept in the regression is equal to the mean donation amount for Other respondents in the means table, the coefficients for Left, Center, and Right have to be added to the intercept to get the mean donation amount for these groups, which matches the values in the means table as well. For example, the average donation amount for Left respondents in the first column can be deduced as follows: \\[E(donationAmount \\mid \\text{Left}) = \\alpha_0 + \\beta_1 \\]\nwhich is about 101.14 + 39.77 = 140.91 dollars, which matches roughly the average donation amount for Left respondents in the means table (aside from differences in rounding). The same logic can be applied to the coefficients for Center and Right.\nIn the second column, however, I change the specification by omitting Right as well, which means Right and Other are pooled into a single reference category. Thus, this is not merely releveling; it is a different model specification because it compares Left and Center to Right and Other. Coefficients therefore change meaning. While the interpretation is still straightforward as before, the estimand is different from the first model.\nMore precisely, the coefficient for the intercept in the second column is now the average donation amount for respondents identifying as Right or Other. This can be confirmed by comparing the coefficient of the intercept with the row for Right or Other in the means table. This also means that the coefficients for Left and Center are now interpreted as the difference in average donation amounts between respondents identifying as either Left or Center on one hand and respondents identifying as Right or Other on the other hand. And since the coefficient of the intercept increased by almost 17 dollars compared to the first column, the coefficients for Left and Center decreased accordingly. For instance, the coefficient for Center is now negative, which reflects the fact that the average donation amount for respondents identifying as Center is 2.35 dollars less than the average donation amount for respondents identifying as Right or Other as the means table shows.\nThe main takeaway is that with pure releveling, fitted values and model fit do not change; only coefficient labels and comparisons do. With pooling reference categories, you change the comparison group and therefore the estimand and the substantive conclusions that you can draw.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "chapters/Categorical_Compositional.html#compositional-predictors",
    "href": "chapters/Categorical_Compositional.html#compositional-predictors",
    "title": "2  Categorical and Compositional Predictors: A Guide",
    "section": "2.3 Compositional Predictors",
    "text": "2.3 Compositional Predictors\nAnother case that faces the same reference category problem, yet is often underemphasized in the literature, is compositional predictors, where values are interpretable only relative to one another and are constrained to sum to a constant, typically 1 or 100%. For instance, researchers are often interested in the effect of the share of the population living in urban areas, the share of the agricultural or industrial workforce, the share of ethnic or religious groups, or the share of the population with a certain level of education on some outcome (to name but a few examples). In all these cases, the share variable is bounded between 0 and 1 and thus has an implicit complement (e.g., rural share = 1 − urban share). For instance, if I have a variable that measures the share of the population living in urban areas, then a value of 0 means everyone is in the complement category, and a value of 1 means no one is in the complement category. This means that the interpretation of the coefficients for share variables is subject to the same problems as before and depends on the number of categories and on which share(s) are omitted from the model.\nTo illustrate the problem, consider party shares in government. Suppose I observe the share of cabinet seats held by left, center, and right parties. Each share is between 0 and 1 and the three shares sum to 1. Because the shares sum to 1, these relationships are always interpreted as shifts in composition (increasing one share necessarily decreases at least one other share). I also observe social spending per capita which serves as the response variable.\nBecause the shares sum to 1, one share must be omitted when I include an intercept. If I omit the right share, the model can be written as:\n\\[\nSocialSpendingpc_i = \\alpha_0 + \\beta_1 left_i + \\beta_2 center_i + \\epsilon_i\n\\]\nHowever, there are a number of different ways to specify the model. Since I can no longer display the mean of the response variable by group (as I did above), I establish a baseline by estimating a model with all party groups and no intercept. After that, I can drop the right share which is the reference category in the first model to demonstrate how both models are connected.\n\nmodel_drop_right &lt;- lm(social_spend_pc ~ left + center, data = data_party_share)\nmodel_no_intercept &lt;- lm(social_spend_pc ~ left + center + right - 1, data = data_party_share)\n\nfmt_coef &lt;- function(cs, term) {\n  if (!term %in% rownames(cs)) {\n    return(\"—\")\n  }\n  sprintf(\"%.2f (%.2f)\", cs[term, 1], cs[term, 2])\n}\n\ncs_drop_right &lt;- coef(summary(model_drop_right))\ncs_no_intercept &lt;- coef(summary(model_no_intercept))\n\nterms &lt;- c(\"(Intercept)\", \"left\", \"center\", \"right\")\n\ntable_out &lt;- data.frame(\n  Term = terms,\n  `No_Intercept` = c(\n    \"—\",\n    fmt_coef(cs_no_intercept, \"left\"),\n    fmt_coef(cs_no_intercept, \"center\"),\n    fmt_coef(cs_no_intercept, \"right\")\n  ),\n  `No_Right` = c(\n    fmt_coef(cs_drop_right, \"(Intercept)\"),\n    fmt_coef(cs_drop_right, \"left\"),\n    fmt_coef(cs_drop_right, \"center\"),\n    \"ref\"\n  ),\n  stringsAsFactors = FALSE\n)\n\nknitr::kable(\n  table_out,\n  align = \"lcc\",\n  caption = \"Party-share regressions with different baselines\"\n)\n\n\nParty-share regressions with different baselines\n\n\nTerm\nNo_Intercept\nNo_Right\n\n\n\n\n(Intercept)\n—\n517.22 (120.67)\n\n\nleft\n6987.47 (114.32)\n6470.26 (199.61)\n\n\ncenter\n1460.77 (113.01)\n943.56 (194.27)\n\n\nright\n517.22 (120.67)\nref\n\n\n\n\n\nThe no-intercept model displays the expected level of social spending per capita if a given party held all cabinet seats (i.e., its share equals 1 and the other shares equal 0). For example, the coefficient for left of about 7000 means that if a government were composed entirely of left parties, I would expect social spending per capita to be almost 7000. Similarly, the coefficients for center and right show the expected spending levels under hypothetical all-center or all-right governments. This specification serves as a diagnostic baseline because it shows the direction and magnitude of each party share’s association with social spending.\nIn the second model with an intercept, the right shares variable is absorbed into the intercept, and each remaining coefficient becomes a contrast against the omitted share rather than a standalone predicted level. The coefficient for intercept is now the expected level of social spending per capita if a government were composed entirely of right parties (i.e., right share equals 1 and the other shares equal 0). The coefficients for left and center are now interpreted as the expected change in social spending per capita for a one-unit (i.e., 100 percentage point) increase in the left (center) share and a simultaneous one-unit decrease in the right share, holding the center (left) share variable constant. Thus, the relationship between the left share and social spending can be mathematically expressed as follows:\n\\[E(SocialSpendingpc \\mid left=1, center=0, right=0) = \\alpha_0 + \\beta_1\\]\nAdding the coefficient for the intercept and the coefficient for left in the second model reproduces the coefficient for left in the no-intercept model. This means that the expected level of social spending per capita if a government were composed entirely of left parties is the sum of the coefficient for intercept and the coefficient for left in the second model, which matches the coefficient for left in the no-intercept model. The same logic applies to the center share.\nThe example illustrates that models with share variables are subject to the same issues of reference categories as models with categorical predictors. The interpretation of the coefficients depends on which share(s) are omitted from the model, and the choice of reference category can change the substantive interpretation of the results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Categorical and Compositional Predictors: A Guide</span>"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "0.3 Prerequisites",
    "text": "0.3 Prerequisites\nYou do not need advanced mathematics to begin.\nRecommended background:\n\nhigh-school algebra,\nbasic comfort with graphs and percentages,\ncuriosity about data and uncertainty.\n\nHelpful but optional:\n\nprior exposure to R (for reading code snippets),\nbasic probability notation.\n\nMost chapters avoid heavy notation. When formulas appear, the focus is on interpretation rather than derivation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Beta to Meaning</span>"
    ]
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "0.4 Disclaimer",
    "text": "0.4 Disclaimer\nAll datasets are simulated. They are designed for learning and do not represent real individuals or institutions.\nCode snippets are included to illustrate ideas, not to provide production-ready software. The examples are written in R, but the conceptual lessons transfer to other languages and tools. You are encouraged to adapt, test, and extend the code for your own use cases.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Beta to Meaning</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-read",
    "href": "index.html#how-to-read",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "0.4 How to read",
    "text": "0.4 How to read\nUse the navigation sidebar to move between chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>So What? Interpreting Regression Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/Interaction_Polynomials.html",
    "href": "chapters/Interaction_Polynomials.html",
    "title": "3  Context Matters: A Guide to Interaction Terms",
    "section": "",
    "text": "3.1 Interaction Terms as Context\nDisclaimer: All data used in this chapter is simulated and does not reflect real-world data. The purpose of this chapter is to illustrate the concept of interaction terms in linear regression models, not to make any claims about the relationship between age, income, and ideological placement.\nResearchers often want to know whether relationships differ across contexts. We use ideological self-placement as the running example. More specifically, I want to know whether age affects left-right self-placement. In addition, I assume that income affects ideological leanings as well, meaning that voters at the lower end of the income distribution are typically more left than voters on the higher end of the income distribution. So, how can interaction terms help us understand the relationship between age, income and ideological placement? Does the relationship between age and ideological placement differ across income levels? If so, how can we model this relationship?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/Interaction_Polynomials.html#interaction-terms-as-context",
    "href": "chapters/Interaction_Polynomials.html#interaction-terms-as-context",
    "title": "3  Context Matters: A Guide to Interaction Terms",
    "section": "",
    "text": "“If You Are Not a Liberal at 25, You Have No Heart. If You Are Not a Conservative at 35 You Have No Brain”\n— Quote Investigator",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/Interaction_Polynomials.html#what-is-an-interaction-term",
    "href": "chapters/Interaction_Polynomials.html#what-is-an-interaction-term",
    "title": "3  Context Matters: A Guide to Interaction Terms",
    "section": "3.2 What is an Interaction Term?",
    "text": "3.2 What is an Interaction Term?\nIn this example, income is measured in tens of thousands, and leftRight is a 0-10 self-placement scale where higher values indicate more right-leaning views.\nWe established in the previous chapter that a coefficient in a regression model represents the average effect of a predictor variable on the response variable, holding all other variables constant. However, this assumes that the effect of the predictor is the same across all contexts. Using a linear regression model with a continuous response variable leftRight and a predictor variable age, we can illustrate this point. Mathematically, our model without an interaction term looks as follows:\n\\[\nleftRight_i = \\alpha + \\beta_1 age_i + \\epsilon_i\n\\tag{3.1}\\]\nEquation 3.1 shows that the marginal effect of a predictor variable in a linear regression model is represented by the coefficient \\(\\beta_1\\). To understand interactions, it is helpful to know that the marginal effect of a predictor can be read directly from the equation because it is the partial derivative of the response variable with respect to the predictor variable. In our simple regression model, the marginal effect of age is \\(\\beta_1\\), which means that for every one-unit increase in age, the expected change in leftRight is \\(\\beta_1\\) units. Mathematically, the partial derivative of Equation 3.1 can be expressed as: \\[\n\\frac{\\partial leftRight_i}{\\partial age_i} = \\beta_1\n\\tag{3.2}\\]\nIn R, this model can be estimated using the lm() function as follows:\n\nsummary(\n  lm(leftRight ~ age, data = data)\n)\n\n\nCall:\nlm(formula = leftRight ~ age, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5059 -0.8757 -0.0084  0.9727  3.0922 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.697214   0.118956   31.08   &lt;2e-16 ***\nage         0.045269   0.002289   19.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.289 on 998 degrees of freedom\nMultiple R-squared:  0.2816,    Adjusted R-squared:  0.2809 \nF-statistic: 391.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nSo, why is this important to understand interaction terms? An interaction term is essentially a multiplicative combination of two predictor variables. When we include an interaction term in our regression model, we simply add two predictors and their product to the equation. Let’s assume that we want to know whether the effect of age on leftRight depends on the income of a respondent. Our regression model with the interaction term would look like this: \\[\nleftRight_i = \\alpha + \\beta_1 age_i + \\beta_2 income_i + \\beta_3 (age_i \\times income_i) + \\epsilon_i\n\\tag{3.3}\\]\nIf we take the partial derivative of Equation 3.3 with respect to age, we get: \\[\n\\frac{\\partial leftRight_i}{\\partial age_i} = \\beta_1 + \\beta_3 income_i\n\\tag{3.4}\\]\nAs Equation 3.4 shows, the effect of age on leftRight is no longer constant but depends on the value of income. The coefficient \\(\\beta_3\\) represents the change in the effect of age for each one-unit increase in income. If \\(\\beta_3\\) is positive, it means that the effect of age on leftRight increases as income increases. Conversely, if \\(\\beta_3\\) is negative, it means that the effect of age on leftRight decreases as income increases.\nIn R, this would look like this:\n\nsummary(\n  lm(leftRight ~ age * income, data = data)\n)\n\n\nCall:\nlm(formula = leftRight ~ age * income, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.45589 -0.55774  0.02019  0.56182  2.66821 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2.4255133  0.2030559  11.945  &lt; 2e-16 ***\nage         0.0224525  0.0038078   5.896 5.08e-09 ***\nincome      0.1625065  0.0264208   6.151 1.12e-09 ***\nage:income  0.0037022  0.0005002   7.401 2.86e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8063 on 996 degrees of freedom\nMultiple R-squared:  0.7196,    Adjusted R-squared:  0.7187 \nF-statistic: 851.9 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n(R adds the constitutive terms automatically when you use the * operator between two variables in the formula.)\nEven though it is more informative to plot the interaction to understand how the effect of age changes across different income levels, we can also look at the coefficients to get a sense of the interaction. First, we can see that the coefficient for age is positive. However, the coefficient for age gives us only the effect of age when income is zero, which is not a meaningful value in this context. A solution here would be to center the income variable around its mean before including it in the regression model. This way, the coefficient for age would represent the effect of age at the average income level.\nSecond, the coefficient for the interaction term age:income is positive, which indicates that the effect of age on leftRight increases as income increases. This means that age is more strongly associated with rightward placement among higher-income respondents than among lower-income respondents. Therefore, the table alone already gives us a pretty good idea of the interaction, but it is often more intuitive to visualize it:\n\n# Fit the model with interaction\nmodel &lt;- lm(leftRight ~ age * income, data = data)\n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model)\nvcov_matrix &lt;- vcov(model)\n\nincome_seq &lt;- seq(2, 12, length.out = 100)\n\neffect &lt;- coefs[2] + coefs[4] * income_seq\n\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           income_seq^2 * vcov_matrix[4, 4] + \n           2 * income_seq * vcov_matrix[2, 4])\n\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(income_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n  xlab = \"Income (tens of thousands)\",\n  ylab = \"Effect of Age on Left-Right Placement\",\n  main = \"Marginal Effect of Age by Income\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(income_seq, rev(income_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.2, 0.2, 0.8, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(income_seq, effect, lwd = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nTo plot the marginal effect of age on leftRight across different income levels, along with 95% confidence intervals, we use the coefficients and variance-covariance matrix from the fitted model. To compute variance for the confidence intervals, we apply the delta method:\n\\[Var(\\beta_1 + \\beta_3 \\cdot income) = Var(\\beta_1) + income^2 \\cdot Var(\\beta_3) + 2 \\cdot income \\cdot Cov(\\beta_1, \\beta_3)\\]\nThe plot shows that the estimated effect of age is positive across the observed income range. This visualization gives us a much clearer picture of how the effect of age changes across income levels compared to a regression table. While age increases rightward placement for all income levels, the effect is much stronger for higher-income respondents than for lower-income respondents.\nAn alternative way to visualize the interaction is to plot predicted values of leftRight across age for different income levels. To do so, we use the 10%, 50%, and 90% quantiles of the income variable to represent low, medium, and high income levels, respectively. We then plot the predicted leftRight values across age for these three income levels, along with confidence intervals.\n\nincome_levels &lt;- as.numeric(quantile(data$income, probs = c(0.1, 0.5, 0.9)))\nincome_labels &lt;- paste0(c(\"P10 = \", \"P50 = \", \"P90 = \"), sprintf(\"%.1f\", income_levels))\nage_grid &lt;- seq(18, 80, length.out = 100)\n\npred_grid &lt;- expand.grid(\n  age = age_grid,\n  income = income_levels\n)\n\npred_ci &lt;- predict(model, newdata = pred_grid, interval = \"confidence\")\npred_grid$leftRight_hat &lt;- pred_ci[, \"fit\"]\npred_grid$ci_lower &lt;- pred_ci[, \"lwr\"]\npred_grid$ci_upper &lt;- pred_ci[, \"upr\"]\n\nplot(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[1]],\n     type = \"l\",\n     lwd = 2,\n     ylim = range(pred_grid$ci_lower, pred_grid$ci_upper),\n     xlab = \"Age\",\n     ylab = \"Predicted Left-Right Placement\",\n    main = \"Predicted Left-Right by Age\\n(at Income P10, P50, and P90)\")\n\npolygon(c(age_grid, rev(age_grid)),\n        c(pred_grid$ci_upper[pred_grid$income == income_levels[1]],\n          rev(pred_grid$ci_lower[pred_grid$income == income_levels[1]])),\n        col = rgb(0, 0, 0, 0.15),\n        border = NA)\n\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[2]], lwd = 2, col = \"blue\")\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[3]], lwd = 2, col = \"red\")\n\npolygon(c(age_grid, rev(age_grid)),\n        c(pred_grid$ci_upper[pred_grid$income == income_levels[2]],\n          rev(pred_grid$ci_lower[pred_grid$income == income_levels[2]])),\n        col = rgb(0, 0, 1, 0.15),\n        border = NA)\n\npolygon(c(age_grid, rev(age_grid)),\n        c(pred_grid$ci_upper[pred_grid$income == income_levels[3]],\n          rev(pred_grid$ci_lower[pred_grid$income == income_levels[3]])),\n        col = rgb(1, 0, 0, 0.15),\n        border = NA)\n\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[1]], lwd = 2, col = \"black\")\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[2]], lwd = 2, col = \"blue\")\nlines(age_grid, pred_grid$leftRight_hat[pred_grid$income == income_levels[3]], lwd = 2, col = \"red\")\n\nlegend(\"topleft\",\n  legend = income_labels,\n       lwd = 2,\n       col = c(\"black\", \"blue\", \"red\"),\n       bty = \"n\")\n\n\n\n\n\n\n\n\nThe plot shows that the predicted leftRight values increase with age for all three income levels, but the slope is steeper for higher-income respondents. This visualization provides an alternative way to understand how the relationship between age and leftRight changes across different income levels, compared to the previous plot of marginal effects. Which visualization is more informative depends on the research question and the audience.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "chapters/Interaction_Polynomials.html#a-brief-digression-polynomial-regression",
    "href": "chapters/Interaction_Polynomials.html#a-brief-digression-polynomial-regression",
    "title": "3  Context Matters: A Guide to Interaction Terms",
    "section": "3.3 A Brief Digression: Polynomial Regression",
    "text": "3.3 A Brief Digression: Polynomial Regression\nWe now extend the interaction idea to nonlinearity. In an interaction, the slope of one variable changes with another variable (x × z). With polynomial terms, the slope of a variable changes with its own level by including powers like x² and x³. Although both are multiplicative in form, polynomial terms are conventionally treated as predictor transformations (basis expansions), not as interaction terms. Using age as an example, our regression model with a cubic term would look like this: \\[\nleftRight_i = \\alpha + \\beta_1 age_i + \\beta_2 age_i^2 + \\beta_3 age_i^3 + \\epsilon_i\n\\tag{3.5}\\]\nThe partial derivative of Equation 3.5 with respect to age is: \\[\n\\frac{\\partial leftRight_i}{\\partial age_i} = \\beta_1 + 2 \\beta_2 age_i + 3 \\beta_3 age_i^2\n\\tag{3.6}\\]\nIn R, a polynomial regression can be estimated by either using the poly() function (or including the polynomial terms manually) as follows:\n\nsummary(\n  lm(leftRight ~ poly(age, degree = 3, raw = TRUE), data = data2)\n)\n\n\nCall:\nlm(formula = leftRight ~ poly(age, degree = 3, raw = TRUE), data = data2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25298 -0.55461  0.01145  0.54104  2.61350 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         3.510e+00  5.398e-01   6.502 1.25e-10 ***\npoly(age, degree = 3, raw = TRUE)1  1.861e-01  3.825e-02   4.865 1.33e-06 ***\npoly(age, degree = 3, raw = TRUE)2 -3.574e-03  8.315e-04  -4.298 1.89e-05 ***\npoly(age, degree = 3, raw = TRUE)3  2.323e-05  5.631e-06   4.126 4.00e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8007 on 996 degrees of freedom\nMultiple R-squared:  0.1354,    Adjusted R-squared:  0.1328 \nF-statistic:    52 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the plot of the marginal effect of age on leftRight would look like this:\n\n# Fit the polynomial regression model with raw polynomials\nmodel_poly &lt;- lm(leftRight ~ age + I(age^2) + I(age^3), data = data2) \n\n# Extract coefficients and variance-covariance matrix\ncoefs &lt;- coef(model_poly)\nvcov_matrix &lt;- vcov(model_poly)\n\n# Create age sequence for prediction\nage_seq &lt;- seq(18, 80, length.out = 100)\n\n# Calculate marginal effect: β₁ + 2*β₂*age + 3*β₃*age²\neffect &lt;- coefs[2] + 2 * coefs[3] * age_seq + 3 * coefs[4] * age_seq^2\n\n# Delta method for standard error:\n# For f(β) = β₁ + 2*β₂*age + 3*β₃*age², gradient is [1, 2*age, 3*age²]\n# SE = sqrt(gradient' * Vcov * gradient)\nse &lt;- sqrt(vcov_matrix[2, 2] + \n           4 * age_seq^2 * vcov_matrix[3, 3] + \n           9 * age_seq^4 * vcov_matrix[4, 4] +\n           4 * age_seq * vcov_matrix[2, 3] +\n           6 * age_seq^2 * vcov_matrix[2, 4] +\n           12 * age_seq^3 * vcov_matrix[3, 4])\n\n# Calculate 95% confidence intervals\nci_lower &lt;- effect - 1.96 * se\nci_upper &lt;- effect + 1.96 * se\n\n# Create plot\nplot(age_seq, effect, \n     type = \"l\", \n     lwd = 2,\n     ylim = c(min(ci_lower), max(ci_upper)),\n     xlab = \"Age\",\n    ylab = \"Marginal Effect of Age on Left-Right Placement\",\n     main = \"Marginal Effect of Age\\n(with 95% Confidence Intervals)\")\n\n# Add confidence interval as shaded region\npolygon(c(age_seq, rev(age_seq)), \n        c(ci_upper, rev(ci_lower)),\n        col = rgb(0.8, 0.2, 0.2, 0.2),\n        border = NA)\n\n# Add reference line at y=0\nabline(h = 0, lty = 2, col = \"gray\", lwd = 1)\n\n# Re-plot the line on top\nlines(age_seq, effect, lwd = 2, col = \"red\")\n\n\n\n\n\n\n\n\nThe plot shows that the marginal effect of age on leftRight is positive but decreasing for younger respondents, becomes not statistically distinguishable from zero for respondents between mid-40s and mid-50s, and becomes positive and significant again for older respondents. This U-shaped pattern illustrates how polynomial regression can capture non-linear relationships between a predictor variable and a response variable. In this example, the relationship between age and leftRight follows a nonlinear pattern with changing slope that can be captured by including polynomial terms in the regression model.\nOf course, other functional forms could be considered to capture different types of non-linear relationships. But the main point is that this example illustrates how polynomial regression can capture non-linear relationships between a predictor variable and a response variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Context Matters: A Guide to Interaction Terms</span>"
    ]
  },
  {
    "objectID": "index.html#what-this-collection-is-and-is-not",
    "href": "index.html#what-this-collection-is-and-is-not",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "0.2 What this collection is (and is not)",
    "text": "0.2 What this collection is (and is not)\nThis is a practice-oriented companion to formal training in statistics.\nIt is:\n\nconcept-first,\nexample-driven,\nfocused on decision-making,\nexplicit about assumptions, limitations, and interpretation.\n\nIt is not:\n\na replacement for a full introductory statistics course,\na complete reference for every method,\na programming manual.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Beta to Meaning</span>"
    ]
  },
  {
    "objectID": "index.html#how-to-read-this-book",
    "href": "index.html#how-to-read-this-book",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "0.5 How to read this book",
    "text": "0.5 How to read this book\nEach article is self-contained, so you can read them in any order. To deepen your understanding, use this collection alongside a comprehensive textbook or a formal course that provides stronger theoretical foundations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Beta to Meaning</span>"
    ]
  },
  {
    "objectID": "index.html#why-this-collection",
    "href": "index.html#why-this-collection",
    "title": "From Beta to Meaning: A Guide to Interpreting Regression Analysis",
    "section": "",
    "text": "framing the right question,\ntranslating that question into a defensible statistical strategy,\nchecking assumptions and model diagnostics,\nevaluating robustness and uncertainty, and\nexplaining results clearly to technical and non-technical audiences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>From Beta to Meaning</span>"
    ]
  }
]